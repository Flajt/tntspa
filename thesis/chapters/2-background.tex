
% Chapter Background
This chapter introduces the background knowledge involved in this thesis. First, Semantic Web Technologies is briefly introduced in section \ref{section:semantic web technologies}, including the notion of RDF in section \ref{subsection:rdf} Linked Data in section \ref{subsection:linked data} and SPARQL in section \ref{subsection:sparql}. Second, section \ref{section:neural machine translation} describes the notion of neural machine translation and involved models and components. Finally, related research is discussed in section \ref{section:related work}.

\section{Semantic Web Technologies} \label{section:semantic web technologies}

% What is Semantic Web and what is the difference between it and the original Web? 
As Tim Berners-Lee et al. stated in \cite{Berners-Lee2001}, the Semantic Web is not an individual web separate from the current one but an extension. In the Semantic Web, there is an important functionality that the machines are able to process the data and information automatically and even understand the data. The semantics of the web pages are well-encoded and displayed to the software agents owned by users or corporates to serve meaningful service. While the original Web consisted largely of documents made up of hypertexts for rendering on the browsers, the meanings of the web page are not well conveyed, thus being difficult for computers to analyse, or users to make higher-level searches. 

% Why does Semantic Web exist since the current Web is a big success? What makes Semantic Web more special than the current Web?

% What are the progress of the Semantc Web at present? What are the current achievements of Semantic Web and what needs to be done?

% What techniques are here to support for Semantic Web? 

\subsection{RDF} \label{subsection:rdf}

\subsection{Linked Data} \label{subsection:linked data}
% What is Linked Data? What is the use of Linked Data? 

\subsection{SPARQL} \label{subsection:sparql}
% What is SPARQL


\section{Neural Machine Translation} \label{section:neural machine translation}

\subsection{Sequence to Sequence Learning}


\subsection{Recurrent Neural Network}


\subsection{Long Short-Term Memory}


\section{Related Work} \label{section:related work}

The primary focus of the investigation in this thesis is the neural network models that can be used to map natural language statements to SPARQL expressions. Despite that such models usually just perform the role of sequence to sequence learning, the specialty of SPARQL as a structured language with strictly defined syntax and vocabulary often lead to highly different experiment results compared to the common machine translation tasks where the source and target sequence are both unstructured. Therefore, we only consider all the research which deployed machine learning methods to map unstructured sequences to structured sequences as the most related work.

\cite{Cai2017} proposed an enhanced encoder-decoder framework for the task of translating natural language to SQL, a similar query language with SPARQL but targeting structured databases instead of knowledge bases. They used not only bleu, but also query accuracy, tuple recall, and tuple precision for measuring the quality of output queries, and achieved good results.

\cite{Soru2018a,Soru2018} proposed a generator-learner-interpreter architecture, namely Neural SPARQL Machines to translate any natural language expression to encoded forms of SPARQL queries. They designed templates with variables that can be filled with instances from certain kinds of concepts in target knowledge base and generated pairs of natural language expression and SPARQL query accordingly. After encoding operators, brackets, and URIs contained in original SPARQL queries, the pairs were fed into a sequence2sequence learner model as the training data. The model was able to predict on unseen natural language sentence, and generate encoding sequence of SPARQL for interpreter to decode. 