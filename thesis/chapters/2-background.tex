
% Chapter Background
This chapter gives an introduction to the background technologies and subfields involved in this thesis. First, Semantic Web Technologies is briefly introduced in section \ref{section:semantic web technologies}, including the notion of RDF in section \ref{subsection:rdf} Linked Data in section \ref{subsection:linked data} and SPARQL in section \ref{subsection:sparql}. Second, section \ref{section:neural machine translation} describes the notion of neural machine translation and involved models and components. Finally, related research is discussed in section \ref{section:related work}.

\section{Semantic Web Technologies} \label{section:semantic web technologies}

% What is Semantic Web and what is the difference between it and the original Web? 
The original Web consisted largely of documents made up of hypertexts for rendering on the browsers, the meanings of the web page are not well conveyed, thus being difficult for computers to analyze, or users to make higher-level searches. As Berners-Lee et al. stated in \cite{Berners-Lee2001}, the Semantic Web is not an individual web separate from the current one but an extension. In the Semantic Web, there is an important functionality that the machines are able to process the data and information automatically and even understand the data. The semantics of the web pages are well-encoded and displayed to the software agents owned by users or corporates to provide meaningful services. In the Semantic Web, the agents from different sources, namely producers and consumers, are able to communicate with each other by exchanging an Ontology which typically contains a taxonomy and a set of inference rules. With the ontology, the computers can define classes, subclasses, and relations among entites on the Web and perform automated reasoning like they "understand" the information \cite{Berners-Lee2001}.

% Why does Semantic Web exist since the current Web is a big success? What makes Semantic Web more special than the current Web?
The World Wide Web has linked more than 10 billion websites, and the useful contents can be served almost instantaneously to the users through search engines. Meanwhile, it is evolving from the web of documents for humans to read to a web of data and information derived from shared semantics. There are various types of programs and intelligent agents around the Web handcrafted for particular tasks, however, they usually possess little ability to deal with heterogeneous information kinds \cite{Shadbolt2006}. There is also a growing need for the integration of data and inforamtion, especially in areas that demand heterogeneous and diverse datasets originating from separate subfields \cite{Shadbolt2006}. A typical use case scenario of the Semantic Web is shown in figure \ref{figure:usecase1}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{placeholder}
\caption{A typical use case scenario of the Semantic Web}
\label{figure:usecase1}
\end{figure}

% What techniques are here to support for Semantic Web? 
A set of technologies are already here to provide a preliminary environment for transforming the current Web into the Semantic Web. The figure \ref{figure:semantic web stack} shows an illustration of the Semantic Web technology stack, where the language in each layer is dependent on the layers below it. These languages have provided a foundation for allowing shared semantics to be integrated into the current documents on the Web, and data to be connected in a more explicit and standardized way. Resource Description Framework (RDF) \cite{Cyganiak2014}, a language located at a lower layer, has provided a foundation for the standardization of the formats of common data. SPARQL, on the other end, is a query language that can be utilized to search and manipulate data in RDF format from diverse sources \cite{Harris2013}. The details of RDF and SPARQL are respectively presented in section \ref{subsection:rdf} and section \ref{subsection:sparql}.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{Semantic-web-stack}
\centering
\caption{Semantic Web Technology Stack}
\label{figure:semantic web stack}
\end{figure}

% What are the progress of the Semantc Web at present? What are the current achievements of Semantic Web and what needs to be done?
The development of standardized technologies for the Semantic Web has promoted the integration of semantics into existing documents and linking of common data across different application domains and even regions. This enables a web of data where the data is connected with typed links, known as Linked Data \cite{Bizer2009}. It uses RDF to link arbitrary entites in the world by making typed statements, and allows complicated queries to be asked in SPARQL. The applications of Linked Data are able to work upon a global and unbound data space, whereas the conventional Web applications normally operate on top of a fixed set of data sources \cite{Bizer2009}. Further information on Linked Data is provided in section \ref{subsection:linked data}.

\subsection{RDF} \label{subsection:rdf}


\subsection{Linked Data} \label{subsection:linked data}
% What is Linked Data? What is the use of Linked Data? 

% Which publishing tools are available for Linked Data?

\subsection{SPARQL} \label{subsection:sparql}
% What is SPARQL


\section{Neural Machine Translation} \label{section:neural machine translation}
% Machine Translation History. Why do we need machine translation? What attempts have been made in machine translation? 
As a global information space, the World Wide Web contains billions of web pages written in languages from various regions of the world. The problem arises when users face the contents they request in a different language from the one they are fluent in. Translation is therefore needed here to lower the language barrier. However, it is evident that human translation does not fit the requirements due to the large quantity of web documents. One alternative solution is Machine Translation. The goal of machine translation is to transform a text from an input language to a target language with the semantic meaning of the text being preserved.

However, due to the complexity of structures, semantics and vocabularies of natural languages, translation is considered a difficulty task for machines. According to \cite{Popovic2012}, the errors occurred in the machine translation output are mainly classified into five base categories: inflectional error, incorrect reordering, missing word, extra word, and lexical ambiguity. There is also debate whether fully high-quality machine translation systems can be achieved \cite{bar1964language}. In addition, lack of context, incomplete common sense knowledge, and ineffectiveness in translating rare words have been major issues that affect the quality of the machine translation systems \cite{okpor2014machine} \cite{Wu2016}.

There have been a large number of approaches in Machine Translation developed over the last years. Currently, the architectures of existing MT systems can be divided into the following categories: 

\begin{itemize}
\item \textbf{Rule-based Machine Translation} (RBMT). RBMT systems usually generate target language text based on an intermediary linguistic representation of the source text and a large set of rules that contain morphological, syntactic, and semantic bilingual mappings. They can be further subdivided into direct, transfer-based, and interlingua-based methods. The performance of RBMT systems, to a certain extent, relies on carefully designed linguistic rules and vast amount of lexicons \cite{Moussallem2017}.
\item \textbf{Statistical Machine Translation} (SMT). SMT systems are developed on the basis of splitting a bilingual text corpora into respective source and translation text pairs. They apply Machine Learning algorithms that compute a statistical model from the corpus given and the model translates each phrase or word at a time based on a probability distribution \cite{Moussallem2017}. In SMT approaches, it usually requires an alignment between source sentence and several target sentences found in each text corpora and vice versa. Such methods suffer in performance when the languages involved have significantly different word orders \cite{okpor2014machine}.
\item \textbf{Example-based Machine Translation} (EBMT). EBMT utilize bilingual corpora like SMT but they translate the text by example sentences. The major limitation of EBMT systems is translation of unknown words \cite{Moussallem2017}. 
\item \textbf{Neural Machine Translation} (NMT). Some argues that NMT is also a statistical approach \cite{Moussallem2017}. In NMT systems, they normally consist of a sequence to sequence model based on neural networks to perform encoding and decoding of the words or characters in the given sentence \cite{Moussallem2017}. During the training of the model, the system steadily learns a representation of both languages in a continuous vector space and the ability to predict a combination of words with higher probability. The approaches in this category currently achieve the state-of-the-art results on several benchmark tests. Their relevant models are the primary focus of the investigation by this thesis.
\item \textbf{Hybrid Machine Translation}. Hybrid approaches essentially leverage the advantages of the methods mentioned above to address their respective limitations and achieve better translation quality. In applications under this category, the hybridization of MT approaches are normally guided by either rule-based or corpus-based statistical systems \cite{costa2015latest}.
\end{itemize}

% The history of neural networks applied in MT


\subsection{Sequence to Sequence Learning}
% What are sequence to sequence learning? What is the role of it in machine learning?

% Which models are proposed in seq2seq learning?

\subsection{Recurrent Neural Network}


\subsection{Long Short-Term Memory}


\section{Related Work} \label{section:related work}

The primary focus of the investigation in this thesis is the neural network models that can be used to map natural language statements to SPARQL expressions. Despite that such models usually just perform the role of sequence to sequence learning, the specialty of SPARQL as a structured language with strictly defined syntax and vocabulary often lead to highly different experiment results compared to the common machine translation tasks where the source and target sequence are both unstructured. Therefore, we only consider all the research which deployed machine learning methods to map unstructured sequences to structured sequences as the most related work.

Cai et al. proposed in \cite{Cai2017} an enhanced encoder-decoder framework for the task of translating natural language to SQL, a similar query language with SPARQL but targeting structured databases instead of knowledge bases. They used not only BLEU \cite{Papineni2002}, but also query accuracy, tuple recall, and tuple precision for measuring the quality of output queries, and achieved good results.

\cite{dong2016language} presented a method based on encoder-decoder model with attention mechanism aimed at translating the input utterances to their logical forms with minimum domain knowledge. Apart from that, they proposed another sequence-to-tree model that has a special decoder better able to capture the hierarchical structure of logical forms. Then, they tested their model on four different datasets and evaluated the results with accuracy as the metric. 

Luz et al. also used a LSTM encoder-decoder model but the purpose is to encode natural language and decode into SPARQL \cite{Luz2018}. Furthermore, they employed a neural probabilistic language model to learn a word vector representation for SPARQL, and used the attention mechanism to associate a vocabulary mapping between natural language and SPARQL. For the experiment, they transformed the logical queries in the traditional Geo880 dataset into equivalent SPARQL form. In terms of evaluation, they adopted two metrics: accuracy and syntactical errors. They further compared their method with several other similar approaches \cite{alagha2015using} \cite{Kaufmann06querix:a} and the comparison showed that they obtained better accuracy results. However, they did not deal with the "out of vocabulary" (OOV) problem and the issue of lexical disambiguation.

In \cite{Soru2018a,Soru2018} Soru et al. proposed a generator-learner-interpreter architecture, namely Neural SPARQL Machines to translate any natural language expression to encoded forms of SPARQL queries. They designed templates with variables that can be filled with instances from certain kinds of concepts in target knowledge base and generated pairs of natural language expression and SPARQL query accordingly. After encoding operators, brackets, and URIs contained in original SPARQL queries, the pairs were fed into a sequence to sequence learner model as the training data. The model was able to predict on unseen natural language sentence, and generate encoding sequence of SPARQL for interpreter to decode. 