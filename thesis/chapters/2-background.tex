
% Chapter Background
This chapter gives an introduction to the background technologies and subfields involved in this thesis. First, Semantic Web technologies are briefly introduced in Section \ref{section:semantic web technologies}, including the notion of RDF in Section \ref{subsection:rdf}, Linked Data in Section \ref{subsection:linked data} and SPARQL in Section \ref{subsection:sparql}. Second, Section \ref{section:neural machine translation} describes the notion of neural machine translation and involved models and components. Finally, related research is discussed in Section \ref{section:related work}.

\section{Semantic Web Technologies} \label{section:semantic web technologies}

% What is Semantic Web and what is the difference between it and the original Web? 
The original Web consisted largely of documents made up of hypertexts for rendering in the browsers, the meanings of the web page are not well conveyed, thus being difficult for computers to analyze, or users to make higher-level searches. As Berners-Lee et al. stated in \cite{Berners-Lee2001}, the Semantic Web is not an individual web separate from the current one but an extension. In the Semantic Web, there is an important functionality that the machines are able to process data and information automatically. The semantics of web pages are well-encoded and displayed to the software agents owned by users or organizations to provide meaningful services. In the Semantic Web, agents from different sources, namely producers and consumers, are able to communicate with each other by exchanging an ontology which typically contains a taxonomy and a set of inference rules. With the ontology, the computers can define classes, subclasses, and relations among entites on the Web and perform automated reasoning as if they "understand" the information \cite{Berners-Lee2001}.

% Why does Semantic Web exist since the current Web is a big success? What makes Semantic Web more special than the current Web?
The World Wide Web has linked more than 10 billion websites, and the useful contents can be delivered almost instantaneously to the users through search engines. Meanwhile, it is evolving from the web of documents for humans to read to a web of data and information derived from shared semantics. There are various types of programs and intelligent agents around the Web handcrafted for particular tasks, however, they usually possess little ability to deal with heterogeneous types of information \cite{Shadbolt2006}. There is also a growing need for the integration of data and information, especially in areas that demand heterogeneous and diverse datasets originating from separate subfields \cite{Shadbolt2006}. A typical use case scenario of the Semantic Web is shown in Figure \ref{figure:usecase1}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{placeholder}
\caption{A typical use case scenario of the Semantic Web}
\label{figure:usecase1}
\end{figure}

% What techniques are here to support for Semantic Web? 
A set of technologies are already here to provide a preliminary environment for transforming the current Web into the Semantic Web. Figure \ref{figure:semantic web stack} shows an illustration of the Semantic Web technology stack, where the language in each layer is dependent on the layers below it. These languages have provided a foundation for allowing shared semantics to be integrated into the current documents on the Web, and data to be connected in a more explicit and standardized way. Resource Description Framework (RDF) \cite{Cyganiak2014}, a language located at a lower layer, has provided a foundation for the standardization of the formats of common data. SPARQL, on the other end, is a query language that can be utilized to search and manipulate data in RDF format from diverse sources \cite{Harris2013}. The details of RDF and SPARQL are respectively presented in section \ref{subsection:rdf} and section \ref{subsection:sparql}.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{Semantic-web-stack}
\centering
\caption{Semantic Web technology stack}
\label{figure:semantic web stack}
\end{figure}

% What are the progress of the Semantc Web at present? What are the current achievements of Semantic Web and what needs to be done?
The development of standardized technologies for the Semantic Web has promoted the integration of semantics into existing documents and linking of common data across different application domains and even regions. This enables a web of data where the data is connected with typed links, known as Linked Data \cite{Bizer2009}. It uses RDF to link arbitrary entites in the world by making typed statements, and allows complicated queries to be submitted in SPARQL. The applications of Linked Data are able to work upon a global and unbound data space, whereas the conventional web applications normally operate on top of a fixed set of data sources \cite{Bizer2009}. Further information on Linked Data is provided in section \ref{subsection:linked data}.

\subsection{RDF} \label{subsection:rdf}

% What is RDF in general? What is the use of it in Semantic Web?
The Resource Description Framework (RDF) is a representation language for defining information on the Web. The Semantic Web Technology Stack (Figure \ref{figure:semantic web stack}) provides the infrastructure to express the meaning of concepts and terms in an organized way that machines can easily process. In 1997, the first RDF specification was defined by the World Wide Web Consortium (W3C) and then it became a W3C recommendation in 1999. Currently, the latest version is RDF 1.1 \cite{Cyganiak2014} published in 2014.

% How does RDF express meanings? i.e. abstract syntax
In RDF, meanings are expressed in triples. A set of triples constitutes an RDF graph, which can be visualized via a diagram containing nodes and directed arcs. Figure \ref{figure:rdf example} demonstrates a simple RDF graph with merely two nodes and one arrow connecting them, which also indicates three components of a triple: subject, predicate, and object. The subject and object represent some resource in the world, and the predicate denotes some relationship. Thus, an RDF triple makes assertions on some relationship of the things it identifies. An RDF graph claims the conjunction of statements encoded by its triples \cite{Cyganiak2014}.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{rdf-graph}
\centering
\caption{An RDF graph with one triple consisting of a subject, a predicate and an object}
\label{figure:rdf example}
\end{figure}

% What does RDF triple consists of specifically?
In the Semantic Web, arbitrary things in the world are denoted as resource and can be referred to by Internationalized Resource Identifier (IRI) or Literals. A datatype is used in addition to the Literal to specify its ranges of value. When no specific resources are identified but implicitly naming some relationship is needed, a blank node is used \cite{Cyganiak2014}. In terms of an RDF triple, the following restriction is defined: 
\medskip
\begin{itemize}
\item the subject is an IRI or a blank node
\item the predicate is an IRI
\item the object is an IRI, a literal or a blank node
\end{itemize}

With the above-introduced notions, we show an example of how RDF can specify the concepts, properties, relations, and corresponding entities existing in the real world. We define the following IRIs\footnote{The IRIs in this example are from \url{https://www.w3.org/TR/rdf11-primer}}: \textit{<http://example.org/bob\#me>} referring to a human entity named Bob, \textit{<http://www.w3.org/1999/02/22-rdf-syntax-ns\#type>} representing "is a" relation, \textit{<http://xmlns.com/foaf/0.1/Person>} specifying a concept denoting the class of person, \textit{<http://schema.org/birthDate>} as a property representing date of birth, and \textit{<http://www.w3.org/2001/XMLSchema\#date>} referring to a data type of date. To express such information: "Bob is a person who is born in 1990-07-04", the following triples are needed:

\begin{center}
\begin{tabular}{c|c|c}
triple 1 & subject & \textit{<http://example.org/bob\#me>}\\
& predicate & \textit{<http://www.w3.org/1999/02/22-rdf-syntax-ns\#type>}\\
& object & \textit{<http://xmlns.com/foaf/0.1/Person>}\\
\hline
triple 2 & subject & \textit{<http://example.org/bob\#me>}\\
& predicate & \textit{<http://schema.org/birthDate>}\\
& object & \textit{"1990-07-04"\textasciicircum\textasciicircum<http://www.w3.org/2001/XMLSchema\#date>}
\end{tabular}
\end{center}

It is noticeable that these IRIs do not share the same prefixes because in an RDF document concepts defined from different sources can be incorporated collaboratively. As a result, the existing knowledge sources are readily combined or extended with new information. Joining the above triples and replacing some IRIs with shorter prefixes we obtain a graph shown in Figure \ref{figure:rdf example2}.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{RDF-Bob}
\centering
\caption{An example RDF graph representing "Bob is a person who is born in 1990-07-04". Some absolute IRIs are edited to be relative with prefixes.}
\label{figure:rdf example2}
\end{figure}

% What are the tools or specifications for writing and publishing RDF?
RDF graphs provide an abstract representation for the knowledge. To write it down in a document, there exists a variety of serialization formats where each of them are designed for different scenarios and purposes. The most used ones are: Turtle \cite{rdfturtle2014}, JSON-LD \cite{jsonld2014}, RDFa \cite{rdfaprimer2013}, and RDF/XML \cite{rdfxml2014}. Referring to the same RDF graph, the documents written in different serialization formats are logically equivalent \cite{schreiber2014rdf}. The data owners can choose from these serialization formats to convert their existing documents into or publish new documents in RDF.

\subsection{SPARQL} \label{subsection:sparql}
% Deep introduction of SPARQL, examples, etc.
SPAQRL Protocol and RDF Query Language (SPARQL) is a structured language similar to SQL but for querying and manipulating RDF graphs \cite{Harris2013}. This thesis primarily focuses on the query capability of SPARQL. A SPARQL query normally consists of a \texttt{SELECT} operator followed by queried objects which are usually denoted by variables or their combinations, a \texttt{WHERE} block containing graph patterns for matching the RDF data store, and optionally extensional operators such as filtering and grouping depending on the requirements. A list of prefixes can be provided in the head for shortening the IRIs involved in the matching graph patterns. The results of SPARQL queries are usually result sets or RDF graphs. 

For instance, to query the RDF graph exhibited in Figure \ref{figure:rdf example2}, a simple SPARQL query with only one single variable and WHERE block can be formulated as in the Listing \ref{listing:sparql query1}.

\begin{lstlisting}[language=SPARQL, label={listing:sparql query1}, caption={A SPARQL query asking for "the persons whose birthday is on 1990 July 4th"}]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX schema: <http://schema.org/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
SELECT ?person
WHERE
{ 
  ?person rdf:type foaf:Person .
  ?person schema:birthDate "1990-07-04"^^xsd:date . 
}
\end{lstlisting}

Note that in SPARQL, prefixes are mandatory in the head of the query to make IRIs resolvable into absolute forms if there are any abbreviations in the body. The query in Listing \ref{listing:sparql query1} simply asks for the persons whose birthday is on 4th of July 1990. Running it on Figure \ref{figure:rdf example2} leads to a result consisting of a single value which can be displayed in tabular form (Table \ref{table:rdf simple result}). The query results can also be exhibited in variable standardized formats including XML, JSON, CSV, and TSV for the benefits of exchanging results in miscellaneous environments \cite{Harris2013}.

\begin{table}[h]
\centering
\begin{tabular}{|c|}
\hline
person \\
\hline
<http://example.org/bob\#me> \\
\hline
\end{tabular}
\caption{Returned result of a SPARQL query on the RDF graph in Figure \ref{figure:rdf example2}}
\label{table:rdf simple result}
\end{table}


Table \ref{table:rdf simple result} indicates that \textit{<http://example.org/bob\#me>} is the only entity that matches the conjunction of both triples in the query graph pattern. If it is likely to have multiple values in the result, one can control the sequence size of the result with \texttt{OFFSET} and \texttt{LIMIT} operators, and further specify the ordering by using \texttt{ORDER BY} operator. In addition, SPARQL supports more advanced operators\footnote{Full capabilities of SPARQL is available at: \url{https://www.w3.org/TR/sparql11-query}} for expressing aggregation, negation, counting, etc. for performing queries with higher complexity against the given RDF data. Some of them are not mentioned here for the reason that it is so far difficult to include all the operators in this thesis. The operators covered in this thesis are listed in Section \ref{section:datasets}.  

\subsection{Linked Data} \label{subsection:linked data}
% What is Linked Data? What is the use of Linked Data? 
RDF as a common data format provides a sound foundation for the unification of information on the traditional Web. While a huge amount of data is available in a standard format, to create a Web of Data that is able to provide more meaningful services, the relationships between data are also important. The collection of interrelated datasets that contain links to each other on the Web are referred to as Linked Data \cite{linkeddataw3c}. Linked Data also refers to a set of best practices to publish, share, and connect the data, information, and knowledge on the Web \cite{Bizer2009}. 

% Which publishing tools are available for Linked Data? What's the current status of Linked Data?
Anyone can create Linked Data and publish it on the Web. There exists a number of ways to do so. One can simply put a static RDF file on a server, or publish relational databases with the help of dedicated tools. A list of tools for editing, publishing, and consuming Linked Data are already publicly available \cite{ldtools}. For large datasets, a SPARQL query service is often needed as well to support broader use cases.

\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{Virtuoso_Linked_Data_Deployment.png} 
\centering
\caption{Current DBpedia data provision architecture \cite{dbparchi} }
\label{figure:dbpedia architecture}
\end{figure}

% Introduce DBpedia and its potential use, applications
DBpedia is one of the most popular large-scale Linked Datasets online. It hosts datasets in RDF representing connected knowledge graphs that are generated by essentially parsing the articles of Wikipedia into structured information. The knowledge graphs on DBpedia not only links the described concepts within Wikipedia but also connects the concepts with other external datasets existing on the Web. In addition, DBpedia is serving a public endpoint upon which applications can post queries in SPARQL to retrieve information in RDF triples for different kinds of purposes. Figure \ref{figure:dbpedia architecture} illustrates the architecture DBpedia is currently adopting for the provision of its RDF data set. In DBpedia, OpenLink Virtuoso Server\footnote{available at: \url{https://virtuoso.openlinksw.com/}} is used to provide support for SPARQL endpoint and HTTP hosting.

\% Add some DBpedia use cases





\section{Neural Machine Translation} \label{section:neural machine translation}
% Machine Translation History. Why do we need machine translation? What attempts have been made in machine translation? 
As a global information space, the World Wide Web contains billions of web pages written in languages from various regions of the world. The problem arises when users face the contents they request in a different language from the one they are fluent in. Translation is therefore needed here to lower the language barrier. However, it is evident that human translation does not fit the requirements due to the large quantity of web documents. One alternative solution is Machine Translation (MT). The goal of machine translation is to transform a text from an input language to a target language with the semantic meaning of the text being preserved.

However, due to the complexity of structures, semantics and vocabularies of natural languages, translation is considered a difficulty task for machines. According to \cite{Popovic2012}, the errors occuring in the machine translation output are mainly classified into five base categories: inflectional error, incorrect reordering, missing word, extra word, and lexical ambiguity. There is also debate whether fully automated high-quality machine translation systems can be achieved \cite{bar1964language}. In addition, lack of context, incomplete common sense knowledge, and ineffectiveness in translating rare words have been major issues that affect the quality of the machine translation systems \cite{okpor2014machine} \cite{Wu2016}.

A large number of approaches have been developed in the area of machine translation over the last years. Currently, the architectures of existing MT systems can be divided into the following categories: 
\medskip  
\begin{itemize}
\item \textbf{Rule-based Machine Translation} (RBMT). RBMT systems usually generate target language text based on an intermediary linguistic representation of the source text and a large set of rules that contain morphological, syntactic, and semantic bilingual mappings. They can be further subdivided into direct, transfer-based, and interlingua-based methods. The performance of RBMT systems, to a certain extent, relies on carefully designed linguistic rules and vast amount of lexicons \cite{Moussallem2017}.
\item \textbf{Statistical Machine Translation} (SMT). SMT systems are developed on the basis of splitting a bilingual text corpus into respective source and translation text pairs. They apply machine learning algorithms that compute a statistical model from the given corpus and the model translates each phrase or word at a time based on a probability distribution \cite{Moussallem2017}. SMT approaches usually require an alignment between source sentence and several target sentences found in each text corpora and vice versa. Such methods suffer in performance when the languages involved have significantly different word orders \cite{okpor2014machine}.
\item \textbf{Example-based Machine Translation} (EBMT). EBMT utilizes bilingual corpora like SMT but they translate the text by example sentences. The major limitation of EBMT systems is translation of unknown words \cite{Moussallem2017}. 
\item \textbf{Neural Machine Translation} (NMT). Some argue that NMT is also a statistical approach \cite{Moussallem2017}. NMT systems commonly consist of a model based on deep neural networks to perform end-to-end translation by words or characters in the given sentence \cite{Moussallem2017}. During the training of the model, the system steadily learns a representation of both languages in a continuous vector space and the ability to predict a combination of words with higher probability. The approaches in this category currently set the new state-of-the-art on several benchmark tests. Their relevant models are the primary focus of the investigation of this thesis.
\item \textbf{Hybrid Machine Translation}. Hybrid approaches essentially leverage the advantages of the methods mentioned above to address their respective limitations and achieve better translation quality. In applications under this category, the hybridization of MT approaches are normally guided by either rule-based or corpus-based statistical systems \cite{costa2015latest}.
\end{itemize}

% The history of neural networks applied in MT
Among these categories, we focus primarily on the Neural Machine Translation methods. The development of NMT systems has gained more interests in recent years since deep neural networks have boosted extraordinary advancements in other areas of Artificial Intelligence such as computer vision \cite{krizhevsky2012imagenet} and speech recognition \cite{dahl2012context}. NMT systems are usually superior in not needing hand-engineering of features that are one of the shortcomings of phrase-based systems \cite{Britz2017}. However, some of the current NMT architectures have disadvantages in requiring large amounts of computation and time for training the deep model \cite{Britz2017}.

So far, many different architectures have been explored in NMT and new methods are constantly beating previous models in some benchmark datasets and achieving higher efficiency in computing. Among these architectures, primary works are listed here. Sutskever et al. \cite{Sutskever2014} and Cho et al. \cite{Cho2014} proposed and deployed an encoder-decoder architecture that contains two models where Recurrent Neural Networks (RNN) (see section \ref{subsection:rnn}) were used. The encoder encodes the input into a fixed-length vector, the decoder then decodes it into a translation. The two models are jointly trained to maximize the likelihood of a target sequence based on the given source sequence. However, the performance of this architecture drops when the length of the input sentence increases. To adress this issue, Bahdanau et al. and Luong et al. presented in \cite{Bahdanau2014,Luong2015} an attention mechanism which serves as an extension to align the encoder and decoder. The acceptance of this mechanism increased the quality of translation significantly. Furthermore, there have been other improvement strategies like bi-directional RNN, beam search, etc. Some variants like Long-Short Term Memory (LSTM) \cite{hochreiter1997long} and Gated Recurrent Unit (GRU) \cite{Cho2014} versions of encoder-decoders have also been investigated. In the mean time, while RNN encoder-decoders consume a lot of computation time on their sequential learning, there are architectures based on Convolutional Neural Networks (CNN) that are able to achieve parallelized computations, thus outperform the former models at a faster speed \cite{Gehring2017}. What's more, Vaswani et al. \cite{Vaswani2017} proposed a self-attention model called the Transformer. This model shows both quality and speed advantages and has achieves state-of-the-art results on multiple translation tasks. In the past years, some novel paradigms for NMT have also emerged like the applications of Generative Adversarial Networks (GAN) in \cite{wu2017adversarial,yang2017improving}.

\subsection{Sequence to Sequence Learning} \label{subsection:seq2seq learning}
% What is sequence to sequence learning? What is the role of it in NMT and ML?

% Which models have been proposed in seq2seq learning?

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{placeholder}
\centering
\caption{A conventional encoder-decoder framework}
\label{figure:encoder-decoder}
\end{figure}

\subsection{Recurrent Neural Network} \label{subsection:rnn}
% Introduction of RNN and technical details

\subsection{Long Short-Term Memory Unit}
% Introduction of LSTM and technical details

\subsection{Convolutional Neural Network}

\subsection{Attention Mechanism}






\section{Related Work} \label{section:related work}

The primary focus of the investigation in this thesis is neural network models that can be used to map natural language statements to SPARQL expressions. Regarding the related work, we consider two main categories: the systems that integrate with NMT approaches and the systems not related to NMT. In the first category, we broaden the range on papers which have deployed machine learning methods to map unstructured sequences to structured sequences for the reason that we observe these methodologies sharing similar principles in learning the prediction of structured sequences like SPARQL. In terms of the second category, we narrow down the choices to those which translate natural language to SPARQL using merely non-NMT approaches on account of referencing their employed datasets, evaluation metrics, and experiment results for comparison. 

\subsection{NMT Systems} \label{subsection:related work with nmt}

Cai et al. \cite{Cai2017} proposed an enhanced encoder-decoder framework for the task of translating natural language to SQL, a similar query language with SPARQL but targeting structured databases instead of knowledge bases. They used not only BLEU \cite{Papineni2002}, but also query accuracy, tuple recall, and tuple precision for measuring the quality of output queries, and achieved good results.

Dong et al. \cite{dong2016language} presented a method based on an encoder-decoder model with attention mechanism aimed at translating the input utterances to their logical forms with minimum domain knowledge. Moreover, they proposed another sequence-to-tree model that has a special decoder better able to capture the hierarchical structure of logical forms. Then, they tested their model on four different datasets and evaluated the results with accuracy as the metric.

Zhong et al. \cite{DBLP:journals/corr/abs-1709-00103} proposed a framework called Seq2SQL for translating natural language questions to SQL. They took LSTM-based encoder-decoder networks as the core model. In order to leverage the structure of the SQL language, they augmented the input question with addition of column names of the queried table and split the decoder into three components, respectively predicting aggregation classifier, column names, and where clause part of a SQL query. As opposed to conventional teacher forcing, they trained the model with reinforcement learning to deal with the problem that queries that execute correct results but do not have exact string matches would be wrongly penalized. To address this issue in the evaluation, they performed analysis with measuring execution accuracy and logical form accuracy of generated queries.

Luz et al. \cite{Luz2018} also used an LSTM encoder-decoder model but the purpose is to encode natural language and decode into SPARQL. Furthermore, they employed a neural probabilistic language model to learn a word vector representation for SPARQL, and used the attention mechanism to associate a vocabulary mapping between natural language and SPARQL. For the experiment, they transformed the logical queries in the traditional Geo880 dataset into equivalent SPARQL form. In terms of evaluation, they adopted two metrics: accuracy and syntactical errors. They further compared their method with several other similar approaches \cite{alagha2015using,Kaufmann06querix:a} and the comparison showed that they obtained better accuracy results. However, they did not deal with the "out of vocabulary" (OOV) problem and the issue of lexical disambiguation.

Soru et al. \cite{Soru2018a,Soru2018} proposed a generator-learner-interpreter architecture, namely Neural SPARQL Machines to translate any natural language expression to encoded forms of SPARQL queries. They designed templates with variables that can be filled with instances from certain kinds of concepts in the target knowledge base and generated pairs of natural language expression and SPARQL query accordingly. After encoding operators, brackets, and URIs contained in original SPARQL queries, the pairs were fed into a sequence to sequence learner model as the training data. The model was able to generalize to unseen natural language sentence, and generate encoding sequence of SPARQL for the interpreter to decode. 

\subsection{Non-NMT Systems} \label{subsection: non-nmt related work}

Dubey et al. \cite{Dubey2016} proposed a framework called \textit{AskNow}, a question answering system targeting at DBpedia. This framework first transforms the questions in English to an intermediary common structure called Normalized Query Structure (NQS), which is later translated into a SPARQL query through a NQS parser and a SPARQL generator. The NQS plays an significant role in carrying the desire, the input information, and their mutual semantic relations in a query. From English query to NQS, they deployed the POS-tagger to retract tags of each tokens in the query and characterized the desire-input dependency relations through semantic analysis based on certain hypotheses. In NQS to SPARQL algorithm, they utilized DBpedia Spotlight \cite{isem2013daiber} and  \cite{Miller:1995:WLD:219717.219748} for query annotation and entity matching, which essentially analyzes the tokens in the query and finds their synonymous entities in the vocabulary of DBpedia. In terms of evaluation, they tested four aspects including syntactic robustness, sensitivity to structural variation, semantic accuracy, and the accuracy of the whole system with precision and recall measurements, and the benchmark data sets QALD \cite{unger:hal-01086472,Unger2015}.



