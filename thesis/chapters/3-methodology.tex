
This chapter mainly describes the models of neural machine translation involved in this thesis for the task of translating natural language to SPARQL and related preliminaries. Then, a measurement BLEU that is typically utilized in automatic machine translation evaluation is described. We also clarify another metric called query accuracy which suits for our task specifically.

\section{Preliminary}

\subsection{SPARQL Encoding} \label{subsection:preprocessing}

Unlike natural language, SPARQL queries are often written in structured forms instead of sequences that are easy to be tokenized. Therefore, the first step of training a Seq2Seq model with SPARQL is to convert SPARQL queries into sequences. To clarify, we encode the original SPARQL query into a sequential form which can be later decoded back. 

We adopted the encoding approach in \cite{Soru2018a}, which basically applies the following operations:
\begin{enumerate}
\item shorten the entities in the query with prefixes
\item replace the built-in symbols with dedicated words connected with underscores
\item shorten the query by replacing the built-in set phrases (e.g. \texttt{ORDER BY}) with one simplified word
\end{enumerate}

These operations can be implemented as a set of replacements and applying them turns an original SPARQL query to a final sequence which contains tokens that are only formed of characters and underscores. An example is shown in Table \ref{table:sparql encoding}. After training, as an encoded form of SPARQL translation is generated, it can be easily decoded back by reverse replacements.

\begin{table}[h]
\begin{tabular}{c | p{0.9\textwidth}}
SPARQL & \begin{lstlisting}[language=SPARQL]
SELECT DISTINCT ?uri 
WHERE { 
<http://dbpedia.org/resource/Sam_Loyd> <http://dbpedia.org/ontology/knownFor> ?uri .
<http://dbpedia.org/resource/Eric_Schiller> <http://dbpedia.org/ontology/knownFor> ?uri . 
}
\end{lstlisting} \\
\hline
Encoded & {\small select distinct var\_uri where brack\_open dbr\_Sam\_Loyd dbo\_knownFor var\_uri sep\_dot dbr\_Eric\_Schiller dbo\_knownFor var\_uri sep\_dot brack\_close}
\end{tabular}
\caption{A SPARQL query is encoded into a sequence of short tokens before treated as the input of the seq2seq model. Thus, the model learns how to translate from natural language texts into sequences of encoded form of SPARQL.}
\label{table:sparql encoding}
\end{table}



\subsection{Input} \label{subsection:preliminary input}

For the input of the NMT models, a sentence or paragraph can be normally separated into three kinds of sequences: characters, subwords, or words. Different methods of splitting sequences lead to differences in vocabularies that are seen by the model and its effectiveness of dealing with rare words. For the task in this thesis, the texts are fed into the models in a word-based fashion on account of the reason that the words contained in the SPARQL vocabulary are mostly SPARQL operators and DBpedia entities that need to be treated as a whole.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{one-hot_encoding}
\centering
\caption{For our task, the source input (left) and target input (right) are one-hot encoding vectors representing word positions from respectively English and SPARQL vocabulary. The output vector is a probability distribution over all words in SPARQL vocabulary.}
\label{figure:one-hot encoding}
\end{figure}

Each word in the sequences of both source input and target input is represented as an one-hot encoding vector, which is a sparse vector in which one element is set to 1 and all other elements are set to 0. The dimension of the one-hot encoding is equal to the size of the specific vocabulary. An example is shown in Figure \ref{figure:one-hot encoding}.

Due to the use of one-hot encoding, every word in the vocabulary is orthogonal to each other. This does not reflect the relevance of some similar or distinct words such as man and king, king and queen. Therefore, in NMT models, the one-hot encoding vectors are usually further mapped into a low-dimensional space where each word is represented as a dense vector called word embedding that holds floating-point values in each element. This mapping can be learned along with the training of the whole model or provided with a pre-trained word embedding matrix.

\subsection{Output}

The output of an NMT model is a sequence of vectors, where the dimension of each vector is the size of the vocabulary for the target language (see Figure \ref{figure:one-hot encoding}). The sum of the entries in each vector is equal to 1, which means the vector is essentially a probability distribution over all the possible words in the target vocabulary. 

The model usually generates the output vectors one by one, where each probability distribution is conditioned on its previous ones. During inference, one can then perform greedy or beam search over this sequence of probability distributions to generate a translation.


\section{Models} \label{section:models}

In this thesis, we mainly focus on investigating three families of deep NMT models, including RNN-based models with an encoder-decoder architecture, the models entirely based on convolutional neural networks, and self-attention models relying on neither RNNs nor CNNs. We consider these three families covering the most advanced research of NMT and best-performing models so far, regardless of any other choices difficult to be classified such as hybrid models. Within each of these categories, one or more representative models are chosen and specified.

\subsection{RNN-based Models}

RNN is the most natural choice for the machine translation task because of its sequential structure. Prior research generally confirms that certain RNN-based models are superior to traditional statistical machine translation methods in translation quality, in spite of some weaknesses on expensive computation and issues of rare words. 

There are many variants in RNN-based models that mostly differ in layer depth, unit type, etc. We choose a simple 2-layer LSTM network which is used in the learner phase of NSpM \cite{Soru2018a} (see Figure \ref{figure:nsm architecture}) as a baseline. On the basis of it, we apply two kinds of attention: global attention \cite{Bahdanau2014} and local attention \cite{Luong2015} to see how much effects the attention mechanism have in our task.

Moreover, we adopt the model proposed by Luong et al. \cite{Luong2015} and the GNMT system proposed by Wu et al. \cite{Wu2016}, both of which achieved state-of-the-art results on natural language translation benchmarks. The former is essentially a deeper LSTM with 4 layers and local attention mechanism. The latter is described in the following.

\subsubsection*{Google's Neural Machine Translation System}

Google's Neural Machine Translation (GNMT) system \cite{Wu2016} is proposed to address several issues existing in past NMT approaches such as lack of robustness and rare words in the input sentences. The model architecture of GNMT is an LSTM network with deep encoder and decoder which have both 8-layer depth as shown in Figure \ref{figure:gnmt architecture}. The attention mechanism is used between the bottom layer of decoder and top layer of encoder. Starting from the third layer, a residual connection is employed between the input and the output of each LSTM cell to remedy the loss of information through deep layers. It essentially adds them together and feeds the result as the input of the next layer. 

\begin{figure}[h]
\includegraphics[width=\textwidth]{gnmt-architecture}
\centering
\caption{The model architecture of GNMT \cite{Wu2016}.}
\label{figure:gnmt architecture}
\end{figure}

Specially, the first layer of the encoder in GNMT is a bi-directional RNN. It is based on the consideration that when output words are being translated left-to-right in the decoder, their closely related words in the source side may appear in distributed positions, which causes the distance between the correct dependency of target and source word to be longer. Although attention mechanism to a great extent has remedied this issue by allowing the decoder to refer to any position of the encoder while decoding, it is practically useful to encode the source information in both directions. The outputs of forward LSTM $ \overrightarrow{x} $ and backward LSTM $ \overleftarrow{x} $ are concatenated together before feeding into the next layer.

To address the issue of rare words, GNMT system adopts the wordpiece model (WPM) that segments the words into wordpieces in the pre-processing stage and establishes a wordpiece vocabulary for the system. During decoding time, the decoder predicts wordpiece sequences which are subsequently converted back to word sequences. WPM is found to be beneficial for the translation accuracy and faster decoding speed \cite{Wu2016}. However, due to the reason that preserving the integration of DBpedia entities in the target vocabulary is needed (mentioned in Section \ref{subsection:preliminary input}), WPM is not used in our experiments. 

\subsection{CNN-based Models} \label{subsection:cnn-based models}

In the machine translation task, CNNs own several advantages over RNNs in the following aspects:
\begin{itemize}
\item Faster training speed because the computations of CNNs allow parallelization over every element in a sequence, whereas the computations in RNN are sequentially dependent on each other.
\item Long-range dependencies have shorter paths when the inputs are processed in a hierarchical multi-layer CNN compared to the chain structure of RNNs. CNN is able to create representations for $ n $ continuous words in $ \mathcal{O}(\frac{n}{k}) $ convolutions with $ k $-width kernels, while RNN needs $ \mathcal{O}(n) $.
\end{itemize}
On the other hand, it also has certain limitations:
\begin{itemize}
\item The input needs to be padded before fed into the model for the reason that CNNs can only process sequences of fixed length.
\item Additional position encoding is required to provide the model with a sense of ordering in the elements being dealt with.
\end{itemize}

\subsubsection*{Convolutional Sequence-to-Sequence}

Convolutional Sequence to Sequence (ConvS2S) \cite{gehring2017convs2s} is a sequence to sequence modeling architecture that depends on CNNs instead of traditional RNNs. 

Figure \ref{figure:convs2s model} depicts the general architecture of the ConvS2S model and how it can be trained. Actually, it still is the architecture of encoder-decoder, and the decoder is also aided by attention mechanism as the RNN-based models. The encoder (placed at top) and the decoder (placed at bottom left) both consist of several stacked convolutional blocks (only one block is drawn in the figure).
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{convs2s-architecture}
\centering
\caption{The demonstration of training the Convolutional Sequence-to-Sequence model \cite{gehring2017convs2s}}
\label{figure:convs2s model}
\end{figure}
Each convolutional block is composed of a one-dimensional convolutional layer (shown in Section \ref{subsection:cnn}) and a following Gated Linear Unit (GLU) as non-linearity. There is a residual connection between the input to the convolutional layer and the output of GLU.


As Figure \ref{figure:convs2s enc dec} displays, the input of the convolutional block can be the output from the previous layer or the word embeddings, which is only the case at the bottom layer. Initially in the training phase, the input for the encoder is $ \textbf{e} = (w_{1}+p_{1},...,w_{m}+p_{m}) $ where $ m $ is the length of an input sentence, $ w_{i} \in \mathbb{R}^{f} $ and $ p_{i} \in \mathbb{R}^{f} $ are the word embedding and the positional embedding of the $ i $-th input element respectively ($ f $ is the embedding size). We denote the output of $ L^{'} $ layers of the encoder convolutional block as $ \textbf{z} = (z^{1},...,z^{L^{'}}) $ where the output of $ l $-th layer is $ z^{l} = (z_{1}^{l},...,z_{m}^{l}) $ and $ l $-th layer is stacked above the $ l-1 $-th layer. We denote the output of $ i $-th convolution operation $ Y = [A\,B] \in \mathbb{R}^{2d} $ where $ A \in \mathbb{R}_{d} $ and $ B \in \mathbb{R}_{d} $ takes each half of $ Y $, then:
\[ z_{i}^{l} = A \otimes \sigma(B) + z_{i}^{l-1} \]
where $ \otimes $ is the point-wise multiplication and $ \sigma $ is the sigmoid function.

Note that because each convolutional block can only receive a fixed number of input elements where it is often a large number that covers the longest sentence in the corpora (i.e. $ m $ is often smaller than the input dimension), the input of each block before the convolution usually needs to be padded with zero vectors (like the grey squares shown in Section \ref{subsection:cnn}).
 
\begin{figure}[h]
\includegraphics[width=\textwidth]{convs2s_enc_dec}
\centering
\caption{The illustration of convolutional blocks in the encoder (left side) and decoder (right side) of ConvS2S model \cite{auliconvs2s}. }
\label{figure:convs2s enc dec}
\end{figure}

Compared to the encoder, each convolutional block in the decoder has one more attention module located after the output of GLU and before the residual connection, which is shown on the right side of Figure \ref{figure:convs2s enc dec}. We denote the output of $ L $ layers of the decoder convolutional block as $ \textbf{h} = (h^{1},...,h^{L^{'}}) $, and the $ l $-th layer's output as $ h^{l} = (h_{1}^{l},...,h_{n}^{l}) $ where $ n $ is, during the training phase the length of target input elements, or during the generation phase the number of current decoding step. In $ l $-th layer, before attention module, the computation is exactly like in the encoder blocks. We denote the $ i $-th intermediate output of GLU as $ v^{l}_{i} $, the final output of $ i $-th decoder state $ h_{i}^{l} $ is then computed as follows:
\begin{align}
d^{l}_{i} &= W^{l}_{d}v^{l}_{i} + b^{l}_{d} + g_{i} \label{eq:decoder state summary}\\
a^{l}_{ij} &= \frac{\exp(d^{l}_{i}\cdot z_{j}^{L^{'}})}{\sum_{t=1}^{m} \exp(d^{l}_{i}\cdot z_{t}^{L^{'}})} \label{eq:attention weight}\\
c^{l}_{i} &= \sum_{j=1}^{m} a^{l}_{ij}(z^{L^{'}}_{j}+e_{j}) \label{eq:conditional input}\\
h^{l}_{i} &= c^{l}_{i} + v^{l}_{i} + h^{l-1}_{i} \label{eq:decoder output}
\end{align}
To compute the attention weights $ a^{l}_{ij} $ between the $ i $-th decoder state and $ j $-th source element, a decoder state summary $ d^{l}_{i} $ is firstly computed in a linear layer with $ v^{l}_{i} $ and the embedding of $ i $-th target element $ g_{i} $ in Eq. \ref{eq:decoder state summary}. After that, the dot-product of $ d^{l}_{i} $ and $ j $-th output of the top encoder block $ z_{j}^{L^{'}} $ is calculated as the attention weights in Eq. \ref{eq:attention weight}. To further let the model refer to the input elements, in Eq. \ref{eq:conditional input}, the conditional input $ c_{i}^{l} $ is computed as the weighted sum of both the encoder outputs and encoder input embeddings $ \textbf{e} $. At last, the output $ h_{i}^{l} $ is the addition of the conditional input, the output of GLU, and the input of this decoder layer from residual connection. 

Since attentions exist in each decoder layer and are computed individually, higher layers have access to the information of which elements the lower layers attended to, which is called multi-step attention. 

Given the output of the top decoder layer $ h^{L} $, a distribution over the possible next target elements at $ i $-th position can be retrieved with a softmax layer $ softmax(W_{o}h_{i}^{L}+b_{o}) $ built upon a linear layer with weights $ W_{o} $ and bias $ b_{o} $. 

This architecture is able to parallelize the computations required during the training phase since the target elements are known beforehand and can be fed to the decoder once. However, during the inference stage where the target elements are not available, the computations in the decoder are still sequential. Nevertheless, full parallelization of the encoder is enough to make this model faster than most of its RNN rivals \cite{gehring2017convs2s}.

\subsection{Self-attention Model}

Recently a new sequence to sequence model that is based neither on RNNs nor CNNs but only on the attention mechanism has emerged and quickly drawn plenty of attention. This kind of model shows a simpler architecture but brings less training cost and is currently claimed to achieve state-of-the-art results on several popular benchmarks such as  English-German and English-French.

\subsubsection*{The Transformer}

The Transformer model \cite{Vaswani2017} is another novel neural machine translation model adopting the encoder decoder structure, without RNNs or CNNs as the primary units but entirely based on the attention mechanism. The traditional RNN models use attention mechanism to connect the encoder and decoder at some time steps. Differently, the Transformer model uses internal stacked self-attention in both encoder and decoder.

\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{multi-attention}
\centering
\caption{The special multi-head attention mechanism (right) in the Transformer model. Each head performs a scaled dot-product attention operation (left) \cite{Vaswani2017}.}
\label{figure:scaled dot-product attention}
\end{figure}

The attention mechanism proposed in the Transformer model is called Multi-Head Attention where each head performs Scaled Dot-Product Attention (see Figure \ref{figure:scaled dot-product attention}) \cite{Vaswani2017}. 

The scaled dot-product attention can be described as a mapping from queries $ Q $ and a set of key-value pairs $ K, V $ to an output attention matrix. Among these vectors, the queries and keys are of the same dimension $ d_{k} $ and the values are of dimension $ d_{v} $. The output of dimension $ d_{v} $ is computed by:
\begin{align} \label{eq:scaled-dot attention}
Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V
\end{align}
where the dot products of the queries and all keys are computed firstly, then scaled by $ \sqrt{d_{k}} $, and finally put through a softmax function to obtain weights on the different positions of values.

Figure \ref{figure:transformer model} displays the architecture of the Transformer model. The encoder and decoder both consist of $ N $ stacked layers where each layer of the encoder is shown on the left half and the decoder on the right half. The connection between them goes from the top layer of the encoder to each layer of the decoder. For all layers as well as their sub-layers in the encoder and decoder, the input and output have the same dimension $ d_{model} $.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{transformer-architecture}
\centering
\caption{The architecture of the Transformer model \cite{Vaswani2017}}
\label{figure:transformer model}
\end{figure}

Each encoder layer consists of two sub-layers. The first is a multi-head attention sub-layer and the second is a point-wise fully connected feed forward sub-layer. Each sub-layer is employed with a residual connection, which means the input to the sub-layer is further passed to the tail and added to the output. After that, the output is normalized. In summary, the output of each sub-layer can be specified by $ Norm(x+Sublayer(x)) $ where $ x $ is the input and $ Sublayer(x) $ indicates the operation performed by this sub-layer. 

For multi-head attention sub-layers (yellow rectangles in Figure \ref{figure:transformer model}), the input $ x $ is composed of three parts $ Q \in \mathbb{R}^{d_{model}} $, $ K \in \mathbb{R}^{d_{model}}$, and $ V \in \mathbb{R}^{d_{model}} $. In sub-layers that aim at performing self-attentions such as the ones in the encoder and the masked sub-layers in the decoder, $ Q $, $K$, and $V $ are identical and from the output of the previous layer. In sub-layers that connect both the encoder and decoder, $ V=K $ and come from the encoder, $ Q $ is from the decoder and the one used in the residual connection. In each multi-head attention sub-layer, there are $ h $ heads that are equivalent to $ h $ parallel attention layers. Each head projects $ Q $, $ K $, and $ V $ to respectively $ d_{k} $, $ d_{k} $, and $ d_{v} $ dimensions and performs a scaled dot-product attention. The outputs gathered from all the heads are concatenated and projected again to a final output of dimension $ d_{model} $. In an equation, it is represented as follows:
\begin{align*}
MultiHead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}
\end{align*}
The output of each head $ head_{i} $ is defined the same as the Eq. \ref{eq:scaled-dot attention}:
\begin{align*}
head_{i} = Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})
\end{align*}
where $ W_{i}^{Q} \in \mathbb{R}^{d_{model} \times d_{k}} $, $ W_{i}^{K} \in \mathbb{R}^{d_{model} \times d_{k}} $, $ W_{i}^{V} \in \mathbb{R}^{d_{model} \times d_{v}} $, and $ W^{O} \in \mathbb{R}^{hd_{v} \times d_{model}} $ are matrices for the corresponding projections.

In terms of fully-connected feed forward sub-layers, $ Sublayer(x) $ means:
\begin{align*}
FeedForward(x) = max(0,xW_{1}+b_{1})W_{2}+b_{2}
\end{align*}
which sequentially propagates the input through one linear transformation, a ReLU activation function, and another linear transformation where the dimension of both linear transformations is $ d_{ff} $. Therefore, the parameters here are $ W_{1} \in \mathbb{R}^{d_{model} \times d_{ff}} $, $ b_{1} \in \mathbb{R}^{d_{ff}} $, $ W_{2} \in \mathbb{R}^{d_{ff} \times d_{model}} $, and $ b_{2} \in \mathbb{R}^{d_{model}} $.

\section{Evaluation Metrics}

Automatic machine translation evaluation is crucial for the development of machine translation system, since human evaluations are normally expensive, longer and subjective to some extents. In this thesis, we use perplexity to see how well a model is being trained, and BLEU to assess how close the translations of the model are to the reference translations.

\subsection{Perplexity}

Perplexity is a common intrinsic measurement for evaluating the quality of a language model based on the cross entropy \cite{koehn2009statistical}. A better language model is one that assigns higher probabilities to the words that actually occur. For a target probability distribution $ p $ and an estimated probability distribution $ q $, the cross entropy is used to assess how close they are and defined as follows:
\begin{align*}
H(p,q) = - \sum_{x} p(x) \log q(x)
\end{align*}
where $ x $ stands for the possible values in the distribution. The perplexity is then defined as the exponentiation of the cross entropy:
\begin{align*}
Perplexity(p,q) = 2^{H(p,q)}
\end{align*}
It is possible to use $ e $ as the base instead of $ 2 $ and it depends on which one is used in the cross entropy.

Specifically for machine translation, the target distribution $ p $ is the one-hot encoding vector of the target vocabulary and $ q $ is obtained from the result of the output softmax layer. In practice, perplexity is calculated per batch or epoch where the cross entropy is averaged over all internal decoding steps beforehand. It is shown in related research \cite{luong2015deep,Wu2016} that perplexity is a good measurement for MT and our results also confirm that source-conditioned perplexity strongly correlates with MT performance.

\subsection{BLEU}

BLEU is now one of the most popular automated metrics in the evaluation of neural machine translation systems. It is noted for its high correlation with human evaluation and low marginal cost for running \cite{Papineni2002}. We choose BLEU as it is the most dominating metric for translation evaluation at present, which brings the advantage of more straightforward comparisons with other successful models and experiments.

N-gram precision is a measurement that is commonly used to reflect the adequacy and fluency of the translation sentence. In a sentence composed of multiple words, an n-gram refers to a contiguous sequence of $ n $ words within it. To compute the precision of n-grams, one just counts up the number of the n-grams which occur in any reference translation, and divide this number by the total number of n-grams in the candidate sentence. This does not check if the candidate translation contains too many duplicate words which merely exist fewer times in reference translation. 

In order to prevent this issue which often occurs in NMT results, a modified version of n-gram precision is proposed in BLEU. To compute, firstly calculate the total number of n-grams in the sentence $ N $. Then pick all the distinct n-grams and for each distinct n-gram $ w $ count the number of times it occurs in the sentence $ c_{w} $ and the maximum number of times it occurs in any single reference as $ m_{w} $. Next, derive a clipped count for each $ w $ by $ c\_clipped_{w} = min(c_{w},m_{w}) $. Finally, the modified n-gram precision is computed by:
\begin{align*}
P = \frac{\sum_{w}c\_clipped_{w}}{N} = \frac{\sum_{w}min(c_{w},m_{w})}{N}
\end{align*}

For example, two candidate translations in Table \ref{table:modified n-gram} are evaluated against the following references:

Reference 1: the cat is on the mat

Reference 2: there is a cat on the mat

When $ n $ equals 1 (unigram), the standard precision of the first candidate is 7/7 since all of the words "the" occur in the references. Whereas the modified version achieved only 2/7, because in this sentence there is only one distinct unigram "the" that has maximum reference count $ m_{w} = 2 $ which is derived from the first reference and total count $ c_{w} = 7 = N $.

\begin{table}[h]
\centering
\caption{The comparison between the standard n-gram precision and the modified version on two candidates. The modified n-gram precision reflects the duplicate words issue on candidate 1 while preserving the standard measurements on candidate 2.}
\label{table:modified n-gram}
\begin{tabular}{c|ccc|ccc}
\multirow{2}{*}{ Candidates } & \multicolumn{3}{c|}{ n-gram precision } & \multicolumn{3}{c}{ modified n-gram precision } \\
\cline{2-7}
& n=1 & n=2 & n=3 & n=1 & n=2 & n=3 \\
\hline
the the the the the the the & 7/7 & 0 & 0 & 2/7 & 0 & 0 \\
\hline
the cat sat on the mat & 5/6 & 3/5 & 1/4 & 5/6 & 3/5 & 1/4 \\
\end{tabular}
\end{table}

Although the modified n-gram precision addresses the duplicates issue, there is a problem that when the candidate translation length is too short, it is likely that the final precision is very high. Therefore, a brevity penalty is introduced as follows:
\begin{align*}
BP = \left\{
\begin{array}{lc}
1 & \text{if } c > r \\
e^{(1-r/c)} & \text{if } c \leq r
\end{array}\right.
\end{align*}
where $ c $ and $ r $ are the length of the candidate translation and the reference. Next, a set of positive weights $ w_{n} $ are assigned where $ \sum_{n=1}^{N} $ equals 1 to take the geometric mean of the scores of the modified n-gram precision $ p_{n} $ with different n-gram sizes up to some $ N $. Experimentally, $ N=4 $ and uniform weights $ w_{n} = 1/N $ performs encouraging results \cite{Papineni2002}. 

Finally, the BLEU score is computed as:
\begin{align*}
\text{BLEU} = BP\cdot \exp(\sum_{n=1}^{N}w_{n}\log p_{n})
\end{align*}

BLEU is used to assess the accuracy of a translation corpus as well. For each translation, there might be multiple corresponding references one of which matches best with the translation in length. In this case, we replace $ c $ and $ r $ in the brevity penalty with the sum of the length of candidate translations and the sum of the best match lengths in references. $ p_{n} $ is replaced as well with the geometric average of the modified n-gram precisions of all the translations. 

