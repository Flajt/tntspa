
This chapter mainly describes the models and frameworks of neural machine translation involved in this thesis for the task of translating natural language to SPARQL and related preliminaries. Then, a measurement BLEU that is typically utilized in automatic machine translation evaluation is described. We also clarify another metric called query accuracy which suits specifically for our task.

\section{Preliminary}

\subsection{Input}

For the input of the NMT models, a sentence or paragraph can be normally separated into three kinds of sequences: characters, subwords, or words. Different methods of splitting sequences lead to differences in vocabularies that are seen by the model and its effectiveness of dealing with rare words. For the task in this thesis, the texts are fed into the models word-based for the reason that the entities in SPARQL which are normally words in the queries need to be treated as a whole.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{one-hot_encoding}
\centering
\caption{For our task, the source input (left) and target input (right) are one-hot encoding vectors representing word positions from respectively English and SPARQL vocabulary. The output vector is a probability distribution over all words in SPARQL vocabulary.}
\label{figure:one-hot encoding}
\end{figure}

Each word in the sequences of both source input and target input is represented in an one-hot encoding vector, which is a sparse vector in which one element is set to 1 and all other elements are set to 0. The dimension of the one-hot encoding is equal to the size of the specific vocabulary. An example is shown in Figure \ref{figure:one-hot encoding}.

Due to the use of one-hot encoding, every word in the vocabulary is orthogonal to each other. This does not reflect the relevance of some similar or distinct words such as man and king, king and queen. Therefore, in NMT models, the one-hot encoding vectors are usually further mapped into a low-dimensional space where each word is represented as a dense vector called word embedding that holds floating-point values in each element. This mapping can be learned along with the training of the whole model or provided with a pre-trained word embedding matrix.

\subsection{Output}

The output of NMT model is a sequence of vectors, where the dimension of each vector is the size of the vocabulary for the target language (see Figure \ref{figure:one-hot encoding}). The sum of the entries in each vector is equal to 1, which means the vector is essentially a probability distribution over all the possible words in the target vocabulary. One can then perform greedy or beam search over this sequence of probability distributions to generate a translation with the highest probabilities.


\section{Models}

In this thesis, we mainly focus on investigating three families of deep NMT models, including RNN-based models with an encoder-decoder architecture, the models entirely based on convolutional neural networks, and self-attention models relying on neither RNNs nor CNNs. We consider these three families covering the most advanced research of NMT and best-performing models so far, regardless of any other choices difficult to be classified such as hybrid models. Within each of these categories, one or more representative models are chosen and specified under.

\subsection{RNN-based Models}

RNN is the most natural choice for machine translation task because of its sequential structure. Prior research generally confirms that certain RNN-based models are superior to traditional statistical machine translation methods in translation quality, in spite of some weaknesses on expensive computation and issues of rare words. 

There are many variants in RNN-based models that mostly differ in layer depth, unit type, etc. We choose a simple 2-layer LSTM network which is used in the learner phase of Neural SPARQL Machine \cite{Soru2018a} as a baseline. On the basis of it, we apply two kinds of attention: global attention \cite{Bahdanau2014} and local attention \cite{Luong2015} to see how much effects the attention mechanism have in our task.

Moreover, we adopt the model proposed by Luong et al. \cite{Luong2015} and the architecture proposed by Wu et al. \cite{Wu2016}, both of which achieved state-of-the-art results on natural language translation benchmarks. The former is essentially a deeper LSTM with 4 layers and local attention mechanism. The latter is described in the following.

\subsubsection*{Google's Neural Machine Translation System}

Google's Neural Machine Translation (GNMT) system \cite{Wu2016} 

\begin{figure}[h]
\includegraphics[width=\textwidth]{gnmt-architecture}
\centering
\caption{The model architecture of GNMT.}
\label{figure:gnmt architecture}
\end{figure}

\subsection{CNN-based Models} \label{subsection:cnn-based models}

In machine translation task, CNNs own several advantages over RNNs in the following aspects:
\begin{itemize}
\item Faster training speed because the computations of CNNs allow parallelization over every element in a sequence, whereas the computations in RNN are sequentially dependent on each other.
\item Long-range dependencies have shorter paths when the inputs are processed in a hierarchical multi-layer CNN compared to the chain structure of RNNs. CNN is able to create representations for $ n $ continuous words in $ \mathcal{O}(\frac{n}{k}) $ convolutions with $ k $-width kernels, while RNN needs $ \mathcal{O}(n) $.
\end{itemize}
On the other hand, it also has certain limitations:
\begin{itemize}
\item The input needs to be padded before fed into the model for the reason that CNNs can only processe sequences of fixed length.
\item Additional position encoding is required to provide the model with a sense of ordering in the elements being dealt with.
\end{itemize}

\subsubsection*{Convolutional Sequence-to-Sequence}

Convolutional Sequence to Sequence (ConvS2S) \cite{gehring2017convs2s} is a sequence to sequence modeling architecture that depends on CNNs instead of traditional RNNs. 

Figure \ref{figure:convs2s model} depicts the general architecture of ConvS2S model and how it can be trained. Actually, it remains the architecture of encoder-decoder, and the decoder is also aided by attention mechanism as the RNN-based models. The encoder (placed at top) and the decoder (placed at bottom left) both consist of several stacked convolutional blocks (only one block is drawn in the figure).
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{convs2s-architecture}
\centering
\caption{The demonstration of training the Convolutional Sequence-to-Sequence model \cite{gehring2017convs2s}}
\label{figure:convs2s model}
\end{figure}
Each convolutional block is composed of a one-dimensional convolutional layer (shown in Section \ref{subsection:cnn}) and a following Gated Linear Unit (GLU) as non-linearity. There is a residual connection between the input to the convolutional layer and the output of GLU.


As Figure \ref{figure:convs2s enc dec} displays, the input of the convolutional block can be the output from the previous layer or the word embeddings, which is only the case at the bottom layer. Initially in the training phase, the input for the encoder is $ \textbf{e} = (w_{1}+p_{1},...,w_{m}+p_{m}) $ where $ m $ is the length of an input sentence, $ w_{i} \in \mathbb{R}^{f} $ and $ p_{i} \in \mathbb{R}^{f} $ are the word embedding and the positional embedding of the $ i $-th input element respectively ($ f $ is the embedding size). We denote the output of $ L^{'} $ layers of encoder convolutional block as $ \textbf{z} = (z^{1},...,z^{L^{'}}) $ where the output of $ l $-th layer is $ z^{l} = (z_{1}^{l},...,z_{m}^{l}) $ and $ l $-th layer is stacked above the $ l-1 $-th layer. We denote the output pf $ i $-th convolution operation $ Y = [A\,B] \in \mathbb{R}^{2d} $, then:
\[ z_{i}^{l} = A \otimes \sigma(B) + z_{i}^{l-1} \]
where $ \otimes $ is the point-wise multiplication and $ \sigma $ is the sigmoid function.

Note that due to the number of accepted input elements in each convolutional block is fixed (i.e. $ m $ is often smaller than the input dimension), the input of each block before the convolution usually needs to be padded with zero vectors (like the grey squares shown in Section \ref{subsection:cnn}).
 
\begin{figure}[h]
\includegraphics[width=\textwidth]{convs2s_enc_dec}
\centering
\caption{The illustration of convolutional blocks in the encoder (left side) and decoder (right side) of ConvS2S model.}
\label{figure:convs2s enc dec}
\end{figure}

Compared to the encoder, each convolutional block in the decoder have one more attention module located after the output of GLU and before the residual connection, which is shown on the right side of Figure \ref{figure:convs2s enc dec}. We denote the output of L layers of decoder convolutional block as $ \textbf{h} = (h^{1},...,h^{L^{'}}) $, and the $ l $-th layer's output as $ h^{l} = (h_{1}^{l},...,h_{n}^{l}) $ where $ n $ is, during the training phase the length of target input elements, or during the generation phase the number of current decoding step. In $ l $-th layer, before attention module, the computation is exactly like in the encoder blocks. We denote the $ i $-th intermediate output of GLU as $ v^{l}_{i} $, the final output of $ i $-th decoder state $ h_{i}^{l} $ is then computed as follows:
\begin{align}
d^{l}_{i} &= W^{l}_{d}v^{l}_{i} + b^{l}_{d} + g_{i} \label{eq:decoder state summary}\\
a^{l}_{ij} &= \frac{\exp(d^{l}_{i}\cdot z_{j}^{L^{'}})}{\sum_{t=1}^{m} \exp(d^{l}_{i}\cdot z_{t}^{L^{'}})} \label{eq:attention weight}\\
c^{l}_{i} &= \sum_{j=1}^{m} a^{l}_{ij}(z^{L^{'}}_{j}+e_{j}) \label{eq:conditional input}\\
h^{l}_{i} &= c^{l}_{i} + v^{l}_{i} + h^{l-1}_{i} \label{eq:decoder output}
\end{align}
To compute the attention weights $ a^{l}_{ij} $ between the $ i $-th decoder state and $ j $-th source element, a decoder state summary $ d^{l}_{i} $ is firstly computed in a linear layer with $ v^{l}_{i} $ and the embedding of $ i $-th target element $ g_{i} $ in Eq. \ref{eq:decoder state summary}. After that, the dot-product of $ d^{l}_{i} $ and $ j $-th output of the top encoder block $ z_{j}^{L^{'}} $ is simply calculated as the attention weights in Eq. \ref{eq:attention weight}. To further let the model refer to the input elements, in Eq. \ref{eq:conditional input}, the conditional input $ c_{i}^{l} $ is computed as the weighted sum of both the encoder outputs and encoder input embeddings $ \textbf{e} $. At last, the output $ h_{i}^{l} $ is the addition of the conditional input, the output of GLU, and the input of this decoder layer from residual connection. 

Since attentions exist in each decoder layer and are computed individually, higher layers have access to the information that which elements the lower layers attended to, which is called multi-step attention. 

Given the output of the top decoder layer $ h^{L} $, a distribution over the possible next target elements at $ i $-th position can be retrieved with a softmax layer $ softmax(W_{o}h_{i}^{L}+b_{o}) $ built upon a linear layer with weights $ W_{o} $ and bias $ b_{o} $. 

This architecture is able to parallel the computations occurred during the training phase since the target elements are known beforehand and can be fed to the decoder once. However, during the inference stage where the target elements are not available, the computations in the decoder are still sequential. Nevertheless, full parallelization of the encoder is enough to make this model faster than most of its RNN rivals \cite{gehring2017convs2s}.

\subsection{Self-attention Models}

\subsubsection*{The Transformer}

The Transformer model \cite{Vaswani2017} is another novel neural machine translation model adopting encoder decoder structure, without RNNs or CNNs as the primary units but entirely based on the attention mechanism. The traditional RNN models use attention mechanism to connect the encoder and decoder at some time steps. Differently, the Transformer model uses internal stacked self-attention in both encoder and decoder.

The attention mechanism used in the Transformer model is called Scaled Dot-Product Attention (see Figure \ref{figure:scaled dot-product attention}) \cite{Vaswani2017}. The function of it can be described as a mapping from queries $ Q $ and a set of key-value pairs where keys and values can be denoted as $ K, V $ to an output matrix. The queries and keys are of dimension $ d_{k} $ and the values are of dimension $ d_{v} $. The output is computed by:
\[ Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V \]
where the dot products of the queries and all keys are computed firstly, scaled by $ \sqrt{d_{k}} $, and then applied a softmax function for obtaining the weights on the values.

\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{multi-attention}
\centering
\caption{}
\label{figure:scaled dot-product attention}
\end{figure}

Figure \ref{figure:transformer model} displays the architecture of the Transformer model, where the left side shows one layer in the encoder and the right half shows one decoder layer.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{transformer-architecture}
\centering
\caption{The architecture of the Transformer model \cite{Vaswani2017}}
\label{figure:transformer model}
\end{figure}

The encoder consists of two sub-layers. The first is a multi-head attention sub-layer and the second is a point-wise fully connected feed forward sub-layer. Each sub-layer is employed a residual connection, which means the input to the sub-layer is further passed to the tail and added to the output. After that, the addition result is normalized. Therefore, the output of each sub-layer can be specified by $ Norm(x+Sublayer(x)) $ where $ x $ is the input and $ Sublayer(x) $ indicates the operation implemented in this sub-layer.


\section{Frameworks}

Three frameworks have been used in this thesis, where two of them are based on TensorFlow \cite{tensorflow2015-whitepaper} and the other one based on PyTorch \cite{paszke2017automatic}.

\subsection{TensorFlow Neural Machine Translation}

\cite{luong17}

\subsection{Facebook AI Research Sequence-to-Sequence Toolkit}

\cite{gehring2017convs2s}

\subsection{Tensor2Tensor}

\cite{tensor2tensor}





\section{Evaluation Metrics}

Automatic machine translation evaluation is crucial for the development of machine translation system, since human evaluations are normally expensive, longer and subjective to some extents. In automatic MT evaluation, for one candidate translation, a score is usually calculated to assess the closeness of it to one or more reference sentences which were mostly human professional translations. 

\subsection{Perplexity}

\subsection{BLEU}

BLEU is now one of the most popular automated metrics in the evaluation of neural machine translation systems. It is noted for its high correlation with human evaluation and low marginal cost for running \cite{Papineni2002}. We choose BLEU as it is the most dominating metric for translation evaluation at present, which brings the advantage of more straightforward comparisons with other successful models and experiments.

The basis of BLEU is a modified n-gram precision measure. In a sentence composed of multiple words, an n-gram refers to a contiguous sequence of n words within it. To compute the precision of n-grams, one just counts up the number of the n-grams which occur in any reference translation, and divide this number by the total number of n-grams in the candidate sentence. This does not check if the candidate translation contains too many duplicate words which merely exist less times in reference translation. The modified version of precision adopted in BLEU addresses this issue. 

In the following example, a candidate translation of poor quality is evaluated with two references. When n equals 1 (i.e. unigram), with standard precision, the candidate achieves 7/7 because every word (unigram) occurs in the first reference, whereas it only achieves 2/7 in modified unigram precision. 

Candidate: the the the the the the the

Reference 1: The cat is on the mat.

Reference 2: There is a cat on the mat.



\subsection{Query Accuracy}


