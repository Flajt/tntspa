@article{rdfaprimer2013,
author = {Herman, Ivan and Adida, Ben and Sporny, Manu and Birbeck, Mark},
title = {RDFa 1.1 Primer - Second Edition},
year = {2013},
journal={World Wide Web Consortium (W3C)},
url = {http://www.w3.org/TR/rdfa-primer/}
}
@article{rdfxml2014,
author = {Gandon, Fabien and Schreiber, Guus},
title = {RDF 1.1 XML Syntax},
year = {2014},
journal={World Wide Web Consortium (W3C)},
url = {http://www.w3.org/TR/rdf-syntax-grammar/}
}
@article{jsonld2014,
author = {Sporny, Manu and Kellogg, Gregg and Lanthaler, Markus},
title = {JSON-LD 1.0},
year = {2014},
journal={World Wide Web Consortium (W3C)},
url = {https://www.w3.org/TR/json-ld/}
}
@article{rdfturtle2014,
author = {Prud'hommeaux, Ericand and Carothers, Gavin},
title = {RDF 1.1 Turtle: Terse RDF Triple Language},
year = {2014},
journal={World Wide Web Consortium (W3C)},
url = {https://www.w3.org/TR/turtle/}
}
@article{schreiber2014rdf,
  title={RDF 1.1 primer. W3C working group note},
  author={Schreiber, Guus and Raimond, Yves},
  journal={World Wide Web Consortium (W3C)},
  year={2014},
  url={https://www.w3.org/TR/rdf11-primer/}
}
@article{DBLP:journals/corr/abs-1709-00103,
  author    = {Victor Zhong and
               Caiming Xiong and
               Richard Socher},
  title     = {Seq2SQL: Generating Structured Queries from Natural Language using
               Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1709.00103},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.00103},
  archivePrefix = {arXiv},
  eprint    = {1709.00103},
  timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-00103},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@article{dahl2012context,
  title={Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition},
  author={Dahl, George E and Yu, Dong and Deng, Li and Acero, Alex},
  journal={IEEE Transactions on audio, speech, and language processing},
  volume={20},
  number={1},
  pages={30--42},
  year={2012},
  publisher={IEEE}
}
@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}
@INPROCEEDINGS{Kaufmann06querix:a,
    author = {Esther Kaufmann and Abraham Bernstein and Renato Zumstein},
    title = {Querix: A Natural Language Interface to Query Ontologies Based on Clarification Dialogs},
    booktitle = {In: 5th ISWC},
    year = {2006},
    pages = {980--981},
    publisher = {Springer}
}
@article{alagha2015using,
  title={Using linguistic analysis to translate arabic natural language queries to SPARQL},
  author={AlAgha, Iyad},
  journal={arXiv preprint arXiv:1508.01447},
  year={2015}
}
@article{dong2016language,
  title={Language to logical form with neural attention},
  author={Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.01280},
  year={2016}
}
@article{costa2015latest,
  title={Latest trends in hybrid machine translation and its applications},
  author={Costa-Jussa, Marta R and Fonollosa, Jos{\'e} AR},
  journal={Computer Speech \& Language},
  volume={32},
  number={1},
  pages={3--10},
  year={2015},
  publisher={Elsevier}
}
@article{okpor2014machine,
  title={Machine translation approaches: issues and challenges},
  author={Okpor, MD},
  journal={International Journal of Computer Science Issues (IJCSI)},
  volume={11},
  number={5},
  pages={159},
  year={2014},
  publisher={International Journal of Computer Science Issues (IJCSI)}
}
@book{bar1964language,
  title={Language and information: Selected essays on their theory and application},
  author={Bar-Hillel, Yehoshua},
  year={1964},
  publisher={Addison-Wesley Reading}
}
@article{Popovic2012,
	author = {Popovi{\ifmmode\acute{c}\else\'{c}\fi}, Maja},
	title = {{Class error rates for evaluation of machine translation output}},
	journal = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
	pages = {71--75},
	year = {2012},
	publisher = {Association for Computational Linguistics},
	url = {https://www.mendeley.com/research-papers/class-error-rates-evaluation-machine-translation-output}
}
@misc{Berners-Lee2001,
abstract = {A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities.},
archivePrefix = {arXiv},
arxivId = {1204.6441},
author = {Berners-Lee, Tim and Hendler, James and Lassila, Ora},
booktitle = {Scientific American},
doi = {10.1038/scientificamerican0501-34},
eprint = {1204.6441},
file = {:Users/yin/Google Drive/TUD Master/Master Thesis/Papers/The Semantic Web.pdf:pdf},
isbn = {9783540762973},
issn = {00368733},
mendeley-groups = {Master Thesis},
month = {may},
number = {5},
pages = {34--43},
pmid = {122},
title = {{The semantic web}},
url = {http://www.nature.com/doifinder/10.1038/scientificamerican0501-34},
volume = {284},
year = {2001}
}
@misc{Cyganiak2014,
author = {Cyganiak, Richard and Wood, David and Lanthaler, Markus},
booktitle = {W3C Recommendation},
mendeley-groups = {Master Thesis},
title = {{RDF 1.1 Concepts and Abstract Syntax}},
url = {https://www.w3.org/TR/rdf11-concepts/},
urldate = {2018-09-13},
year = {2014}
}
@article{Bizer2009,
abstract = {The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions -the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
doi = {10.4018/jswis.2009081901},
eprint = {arXiv:1011.1669v3},
file = {:Users/yin/Google Drive/TUD Master/Master Thesis/Papers/Linked-Data---The-Story-So-Far.pdf:pdf},
isbn = {9781605580852},
issn = {1552-6283},
journal = {International Journal on Semantic Web and Information Systems},
keywords = {data exploration,data sharing,linked data,semantic web,web of data},
mendeley-groups = {Master Thesis},
number = {3},
pages = {1--22},
pmid = {16},
title = {{Linked Data - The Story So Far}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/jswis.2009081901},
volume = {5},
year = {2009}
}
@inproceedings{Auer2007,
abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information can be made available on the Web for humans and machines. We describe some emerging applications from the DBpedia community and show how website operators can reduce costs by facilitating royalty-free DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data sources.},
author = {Auer, S{\"{o}}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-76298-0_52},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Auer et al. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf:pdf},
isbn = {3540762973},
issn = {03029743},
mendeley-groups = {Master Thesis,Master Thesis/Introduction},
month = {nov},
pages = {722--735},
publisher = {Springer, Berlin, Heidelberg},
title = {{DBpedia: A nucleus for a Web of open data}},
url = {http://link.springer.com/10.1007/978-3-540-76298-0{\_}52},
volume = {4825 LNCS},
year = {2007}
}
@techreport{Harris2013,
abstract = {RDF is a directed, labeled graph data format for representing information in the Web. This specification defines the syntax and semantics of the SPARQL query language for RDF. SPARQL can be used to express queries across diverse data sources, whether the data is stored natively as RDF or viewed as RDF via middleware. SPARQL contains capabilities for querying required and optional graph patterns along with their conjunctions and disjunctions. SPARQL also supports aggregation, subqueries, negation, creating values by expressions, extensible value testing, and constraining queries by source RDF graph. The results of SPARQL queries can be result sets or RDF graphs.},
author = {Harris, Steve and Seaborne, Andy},
booktitle = {W3C Recommendation},
doi = {citeulike-article-id:2620569},
isbn = {0752820907},
mendeley-groups = {Master Thesis,Master Thesis/Introduction},
pmid = {31},
title = {{SPARQL 1.1 Query Language}},
year = {2013}
}
@misc{Shadbolt2006,
abstract = {The original Scientific American article on the Semantic Web appeared in 2001. It described the evolution of a Web that consisted largely of documents for humans to read to one that included data and information for computers to manipulate. The Semantic Web is a Web of actionable information--information derived from data through a semantic theory for interpreting the symbols.This simple idea, however, remains largely unrealized. Shopbots and auction bots abound on the Web, but these are essentially handcrafted for particular tasks; they have little ability to interact with heterogeneous data and information types. Because we haven't yet delivered large-scale, agent-based mediation, some commentators argue that the Semantic Web has failed to deliver. We argue that agents can only flourish when standards are well established and that the Web standards for expressing shared meaning have progressed steadily over the past five years. Furthermore, we see the use of ontologies in the e-science community presaging ultimate success for the Semantic Web--just as the use of HTTP within the CERN particle physics community led to the revolutionary success of the original Web. This article is part of a special issue on the Future of AI.},
archivePrefix = {arXiv},
arxivId = {1204.6441},
author = {Shadbolt, Nigel and Hall, Wendy and Berners-Lee, Tim},
booktitle = {IEEE Intelligent Systems},
doi = {10.1109/MIS.2006.62},
eprint = {1204.6441},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Staab - 2006 - Editor A growing need for data integration.pdf:pdf},
isbn = {9783540762973},
issn = {15411672},
mendeley-groups = {Master Thesis,Master Thesis/Introduction},
number = {3},
pages = {96--101},
pmid = {122},
title = {{The semantic web revisited}},
url = {http://www.w3.org/2000/10/swap/pim/contact{\#}Person},
volume = {21},
year = {2006}
}
@article{Kaiser2017,
abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
archivePrefix = {arXiv},
arxivId = {1706.05137},
author = {Kaiser, Lukasz and Gomez, Aidan N and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
doi = {10.1007/s11263-015-0816-y},
eprint = {1706.05137},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Kaiser et al. - Unknown - One Model To Learn Them All.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
mendeley-groups = {Master Thesis},
pmid = {16190471},
title = {{One Model To Learn Them All}},
url = {https://github.com/tensorflow/tensor2tensor http://arxiv.org/abs/1706.05137},
year = {2017}
}
@article{Vaswani2018,
abstract = {Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.},
archivePrefix = {arXiv},
arxivId = {1803.07416},
author = {Vaswani, Ashish and Bengio, Samy and Brevdo, Eugene and Chollet, Francois and Gomez, Aidan N and Gouws, Stephan and Jones, Llion and Kaiser, {\L}ukasz and Kalchbrenner, Nal and Parmar, Niki and Sepassi, Ryan and Shazeer, Noam and Uszkoreit, Jakob},
eprint = {1803.07416},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2018 - Tensor2Tensor for Neural Machine Translation.pdf:pdf},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
title = {{Tensor2Tensor for Neural Machine Translation}},
url = {https://arxiv.org/pdf/1803.07416.pdf http://arxiv.org/abs/1803.07416},
year = {2018}
}
@article{T.HeathM.Hepp2009,
abstract = {The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions-the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.},
author = {{T. Heath, M. Hepp}, C. Bizer},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Bizer, Heath - Unknown - Special Issue on Linked Data.pdf:pdf},
journal = {International Journal on Semantic Web and Information Systems (IJSWIS)},
keywords = {Data Exploration,Data Sharing,Linked Data,Semantic Web,Web of Data},
mendeley-groups = {Master Thesis},
title = {{Special Issue on Linked Data}},
url = {http://linkeddata.org/docs/ijswis-special-issue},
volume = {5},
year = {2009}
}
@article{Cai2017,
abstract = {Machine translation is going through a radical revolution, driven by the explosive development of deep learning techniques using Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In this paper, we consider a special case in machine translation problems, targeting to convert natural language into Structured Query Language (SQL) for data retrieval over relational database. Although generic CNN and RNN learn the grammar structure of SQL when trained with sufficient samples, the accuracy and training efficiency of the model could be dramatically improved, when the translation model is deeply integrated with the grammar rules of SQL. We present a new encoder-decoder framework, with a suite of new approaches, including new semantic features fed into the encoder, grammar-aware states injected into the memory of decoder, as well as recursive state management for sub-queries. These techniques help the neural network better focus on understanding semantics of operations in natural language and save the efforts on SQL grammar learning. The empirical evaluation on real world database and queries show that our approach outperform state-of-the-art solution by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1711.06061},
author = {Cai, Ruichu and Xu, Boyan and Yang, Xiaoyan and Zhang, Zhenjie and Li, Zijian and Liang, Zhihao},
eprint = {1711.06061},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Cai et al. - 2017 - An Encoder-Decoder Framework Translating Natural Language to Database Queries.pdf:pdf},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
title = {{An Encoder-Decoder Framework Translating Natural Language to Database Queries}},
url = {https://arxiv.org/pdf/1711.06061.pdf http://arxiv.org/abs/1711.06061},
year = {2017}
}
@article{Sander2014,
abstract = {We present an implemented approach to transform natu-ral language sentences into SPARQL, using background knowledge from ontologies and lexicons. Therefore, el-igible technologies and data storage possibilities are an-alyzed and evaluated. The contributions of this paper are twofold. Firstly, we describe the motivation and current needs for a natural language access to industry data. We describe several scenarios where the proposed solution is required. Resulting in an architectural approach based on automatic SPARQL query construction for effective natural language queries. Secondly, we analyze the per-formance of RDBMS, RDF and Triple Stores for the knowledge representation. The proposed approach will be evaluated on the basis of a query catalog by means of query efficiency, accuracy, and data storage perfor-mance. The results show, that natural language access to industry data using ontologies and lexicons, is a sim-ple but effective approach to improve the diagnosis pro-cess and the data search for a broad range of users. Fur-thermore, virtual RDF graphs do support the DB-driven knowledge graph representation process, but do not per-form efficient under industry conditions in terms of per-formance and scalability.},
author = {Sander, Malte and Waltinger, Ulli and Roshchin, Mikhail and Runkler, Thomas},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Sander et al. - 2014 - Ontology-Based Translation of Natural Language Queries to SPARQL.pdf:pdf},
isbn = {9781577356967},
journal = {2014 AAAI Fall Symposium Ontology-Based},
keywords = {AAAI Technical Report FS-14-06},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
pages = {42--48},
title = {{Ontology-Based Translation of Natural Language Queries to SPARQL}},
url = {www.aaai.org https://www.aaai.org/ocs/index.php/FSS/FSS14/paper/viewFile/9172/9084},
year = {2014}
}
@inproceedings{Pradel2013,
abstract = {Our purpose is to provide end-users with a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. In this article we justify the postulate sup- porting our work which claims that queries issued by real life end-users are variations of a few typical query families. We also explain how our approach is designed to be adaptable to different user languages. Evalua- tions on the QALD-3 data set have shown the relevancy of the approach. 1},
author = {Pradel, Camille and Haemmerl{\'{e}}, Ollivier and Hernandez, Nathalie},
booktitle = {CEUR Workshop Proceedings},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Pradel, Haemmerl{\'{e}}, Hernandez - Unknown - Natural Language Query Interpretation into SPARQL Using Patterns.pdf:pdf},
issn = {16130073},
mendeley-groups = {Master Thesis,Master Thesis/Background},
title = {{Natural language query interpretation into SPARQL using patterns}},
url = {http://www.w3.org/TR/rdf-sparql-query/},
volume = {1034},
year = {2013}
}
@article{Yang2017,
abstract = {This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences (i.e., the golden target sentences), And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.},
archivePrefix = {arXiv},
arxivId = {1703.04887},
author = {Yang, Zhen and Chen, Wei and Wang, Feng and Xu, Bo},
doi = {10.18653/v1/N18-1122},
eprint = {1703.04887},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - Unknown - Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets.pdf:pdf},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
title = {{Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets}},
url = {https://github.com/tensorflow/tensor2tensor http://arxiv.org/abs/1703.04887},
year = {2017}
}
@article{wu2017adversarial,
  title={Adversarial neural machine translation},
  author={Wu, Lijun and Xia, Yingce and Zhao, Li and Tian, Fei and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1704.06933},
  year={2017}
}
@article{yang2017improving,
  title={Improving neural machine translation with conditional sequence generative adversarial nets},
  author={Yang, Zhen and Chen, Wei and Wang, Feng and Xu, Bo},
  journal={arXiv preprint arXiv:1703.04887},
  year={2017}
}
@article{Qi2018,
abstract = {The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases -- providing gains of up to 20 BLEU points in the most favorable setting.},
archivePrefix = {arXiv},
arxivId = {1804.06323},
author = {Qi, Ye and Sachan, Devendra Singh and Felix, Matthieu and Padmanabhan, Sarguna Janani and Neubig, Graham},
eprint = {1804.06323},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Qi et al. - Unknown - When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation.pdf:pdf},
mendeley-groups = {Master Thesis},
title = {{When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?}},
url = {https://github.com/neulab/xnmt/ http://arxiv.org/abs/1804.06323},
year = {2018}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
doi = {10.1017/S0140525X16001837},
eprint = {1706.03762},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
isbn = {9781577357384},
issn = {0140-525X},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
month = {jun},
pmid = {1000303116},
title = {{Attention Is All You Need}},
url = {http://arxiv.org/abs/1706.03762},
year = {2017}
}
@article{Barone2017,
abstract = {It has been shown that increasing model depth improves the quality of neural machine translation. However, different architectural variants to increase model depth have been proposed, and so far, there has been no thorough comparative study. In this work, we describe and evaluate several existing approaches to introduce depth in neural machine translation. Additionally, we explore novel architectural variants, including deep transition RNNs, and we vary how attention is used in the deep decoder. We introduce a novel "BiDeep" RNN architecture that combines deep transition RNNs and stacked RNNs. Our evaluation is carried out on the English to German WMT news translation dataset, using a single-GPU machine for both training and inference. We find that several of our proposed architectures improve upon existing approaches in terms of speed and translation quality. We obtain best improvements with a BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU over a strong shallow baseline. We release our code for ease of adoption.},
archivePrefix = {arXiv},
arxivId = {1707.07631},
author = {Barone, Antonio Valerio Miceli and Helcl, Jindřich and Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
eprint = {1707.07631},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Valerio et al. - Unknown - Deep Architectures for Neural Machine Translation.pdf:pdf},
mendeley-groups = {Master Thesis},
title = {{Deep Architectures for Neural Machine Translation}},
url = {http://www.statmt.org/wmt17/ http://arxiv.org/abs/1707.07631},
year = {2017}
}
@article{Chen2018,
abstract = {The past year has witnessed rapid ad-vances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolu-tional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches con-sists of a fundamental architecture accom-panied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this pa-per, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key mod-eling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English→French and English→German tasks. Second, we analyze the properties of each fundamen-tal seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models ob-tain further improvements, outperforming the RNMT+ model on both benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1804.09849},
author = {Chen, Mia Xu and Firat, Orhan and Banpna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and NikiParmar and Schuster, Mike and Chen, Zhifeng and Wu, Yonghui and Hughes, Macduff},
doi = {arXiv:1804.09849v1},
eprint = {1804.09849},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - The Best of Both Worlds Combining Recent Advances in Neural Machine Translation.pdf:pdf},
journal = {arXiv},
mendeley-groups = {Master Thesis},
pages = {1--12},
title = {{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation}},
url = {https://github.com/tensorflow/tensor2tensor},
year = {2018}
}
@article{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
archivePrefix = {arXiv},
arxivId = {1702.00764},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wj},
doi = {10.3115/1073083.1073135},
eprint = {1702.00764},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Papineni et al. - Unknown - BLEU a Method for Automatic Evaluation of Machine Translation.pdf:pdf},
isbn = {1-55860-883-4},
issn = {00134686},
journal = {{\ldots} of the 40Th Annual Meeting on {\ldots}},
mendeley-groups = {Master Thesis},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://www.aclweb.org/anthology/P02-1040.pdf http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@inproceedings{Freitag2017,
abstract = {The basic concept in Neural Machine Translation (NMT) is to train a large Neu-ral Network that maximizes the transla-tion performance on a given parallel cor-pus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of ac-tive candidates at each time step. First, this simple search is less adaptive as it also ex-pands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increas-ing the beam size until no performance im-provement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexi-ble beam search strategy whose candidate size may vary at each time step depend-ing on the candidate scores. We speed up the original decoder by up to 43{\%} for the two language pairs German→English and Chinese→English without losing any translation quality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1702.01806v2},
author = {Freitag, Markus and Al-Onaizan, Yaser},
booktitle = {NMT},
eprint = {arXiv:1702.01806v2},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Freitag, Al-Onaizan - 2017 - Beam Search Strategies for Neural Machine Translation.pdf:pdf},
mendeley-groups = {Master Thesis},
month = {feb},
title = {{Beam Search Strategies for Neural Machine Translation}},
url = {http://arxiv.org/abs/1702.01806 https://arxiv.org/pdf/1702.01806.pdf},
year = {2017}
}
@article{Britz2017,
abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.},
archivePrefix = {arXiv},
arxivId = {1703.03906},
author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
eprint = {1703.03906},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Britz et al. - Unknown - Massive Exploration of Neural Machine Translation Architectures.pdf:pdf},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
title = {{Massive Exploration of Neural Machine Translation Architectures}},
url = {https://github.com/moses- http://arxiv.org/abs/1703.03906},
year = {2017}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
mendeley-groups = {Master Thesis},
month = {jun},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Kalchbrenner2013,
abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is {\textgreater} 43{\%} lower than that of state-of-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and mean- ing of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Kalchbrenner, Nal and Blunsom, Phil},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Kalchbrenner, Blunsom - Unknown - Recurrent Continuous Translation Models.pdf:pdf},
isbn = {9781937284978},
issn = {0147-006X},
journal = {Emnlp},
mendeley-groups = {Master Thesis},
number = {October},
pages = {1700--1709},
pmid = {14527267},
publisher = {Association for Computational Linguistics},
title = {{Recurrent Continuous Translation Models}},
url = {https://www.aclweb.org/anthology/D13-1176},
year = {2013}
}
@article{Luz2018,
abstract = {Semantic parsing is the process of mapping a natural language sentence into a formal representation of its meaning. In this work we use the neural network approach to transform natural language sentence into a query to an ontology database in the SPARQL language. This method does not rely on handcraft-rules, high-quality lexicons, manually-built templates or other handmade complex structures. Our approach is based on vector space model and neural networks. The proposed model is based in two learning steps. The first step generates a vector representation for the sentence in natural language and SPARQL query. The second step uses this vector representation as input to a neural network (LSTM with attention mechanism) to generate a model able to encode natural language and decode SPARQL.},
archivePrefix = {arXiv},
arxivId = {1803.04329},
author = {Luz, Fabiano Ferreira and Finger, Marcelo},
eprint = {1803.04329},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Ferreira Luz, Finger - Unknown - Semantic Parsing Natural Language into SPARQL Improving Target Language Representation with Neural A(2).pdf:pdf},
mendeley-groups = {Master Thesis},
title = {{Semantic Parsing Natural Language into SPARQL: Improving Target Language Representation with Neural Attention}},
url = {https://arxiv.org/pdf/1803.04329.pdf http://arxiv.org/abs/1803.04329},
year = {2018}
}
@article{Mikolov2017,
abstract = {Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.},
archivePrefix = {arXiv},
arxivId = {1712.09405},
author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
eprint = {1712.09405},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2017 - Advances in Pre-Training Distributed Word Representations.pdf:pdf},
keywords = {fastText pre-trained vectors},
mendeley-groups = {Master Thesis},
mendeley-tags = {fastText pre-trained vectors},
month = {dec},
title = {{Advances in Pre-Training Distributed Word Representations}},
url = {http://arxiv.org/abs/1712.09405},
year = {2017}
}
@article{Luong2015,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
doi = {10.18653/v1/D15-1166},
eprint = {1508.04025},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Luong, Pham, Manning - 2015 - Effective Approaches to Attention-based Neural Machine Translation.pdf:pdf},
isbn = {9781941643327},
issn = {10495258},
journal = {EMNLP},
keywords = {()},
mendeley-groups = {Master Thesis},
number = {September},
pages = {11},
pmid = {14527267},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {http://nlp.stanford.edu/projects/nmt. http://arxiv.org/abs/1508.04025},
year = {2015}
}
@inproceedings{Sorokin2017,
abstract = {In this paper we present a factoid question answering system for participation in Task 4 of the QALD-7 shared task. Our system is an end-to-end neural architecture for learning a semantic representation of the input question. It iteratively generates representations and uses a convolutional neural network (CNN) model to score them at each step. We take the semantic representation with the highest final score and execute it against Wikidata to retrieve the answers. We show on the Task 4 data set that our system is able to successfully generalize to new data.},
author = {Sorokin, Daniil and Gurevych, Iryna},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-319-69146-6_7},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Sorokin, Gurevych - 2017 - End-to-end representation learning for question answering with weak supervision.pdf:pdf},
isbn = {9783319691459},
issn = {18650929},
keywords = {Convolutional neural networks,Question-answering,Representation learning,Semantic parsing,Semantic web,Weak supervision},
mendeley-groups = {Master Thesis},
pages = {70--83},
publisher = {Springer, Cham},
title = {{End-to-end representation learning for question answering with weak supervision}},
url = {http://link.springer.com/10.1007/978-3-319-69146-6{\_}7},
volume = {769},
year = {2017}
}
@misc{Lake2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
booktitle = {Behavioral and Brain Sciences},
doi = {10.1017/S0140525X16001837},
eprint = {1604.00289},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf:pdf},
isbn = {9781577357384},
issn = {14691825},
mendeley-groups = {Master Thesis},
month = {jul},
pages = {1--101},
pmid = {1000303116},
title = {{Building Machines That Learn and Think Like People}},
url = {http://arxiv.org/abs/1607.04606},
year = {2016}
}
@misc{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {14764687},
keywords = {Computer science,Mathematics and computing},
mendeley-groups = {Master Thesis},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@inproceedings{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NIPS)},
doi = {v5},
eprint = {1503.08895},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Sukhbaatar et al. - 2015 - End-To-End Memory Networks.pdf:pdf},
isbn = {1551-6709},
issn = {10495258},
mendeley-groups = {Master Thesis},
month = {mar},
pmid = {9377276},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {1410.5401},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
isbn = {0028-0836},
issn = {2041-1723},
mendeley-groups = {Master Thesis},
month = {oct},
pmid = {18958277},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
mendeley-groups = {Master Thesis},
month = {sep},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2014}
}
@article{Gehring2017,
abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
annote = {introduced an architecture based entirely on convolutional neural networks.
advantages over RNN: allow parallel computation on every element in a sequence},
archivePrefix = {arXiv},
arxivId = {1705.03122},
author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
doi = {10.18653/v1/P16-1220},
eprint = {1705.03122},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Gehring et al. - Unknown - Convolutional Sequence to Sequence Learning.pdf:pdf},
isbn = {9781510827585},
issn = {1938-7228},
mendeley-groups = {Master Thesis},
title = {{Convolutional Sequence to Sequence Learning}},
url = {https://arxiv.org/pdf/1705.03122.pdf http://arxiv.org/abs/1705.03122},
year = {2017}
}
@article{Soru2018,
abstract = {Research on question answering with knowledge base has recently seen an increasing use of deep architectures. In this extended abstract, we study the application of the neural machine translation paradigm for question parsing. We employ a sequence-to-sequence model to learn graph patterns in the SPARQL graph query language and their compositions. Instead of inducing the programs through question-answer pairs, we expect a semi-supervised approach, where alignments between questions and queries are built through templates. We argue that the coverage of language utterances can be expanded using late notable works in natural language generation.},
annote = {Published version of SPARQL as a Foreign Language
Experiment results:
The encoding of SPARQL highly influences the learning.
More complex templates contributed to richer training set and more correctly parsed questions
The most frequent errors are because of entity name collisions and out of vocabulary words.},
archivePrefix = {arXiv},
arxivId = {1806.10478},
author = {Soru, Tommaso and Marx, Edgard and Valdestilhas, Andr{\'{e}} and Esteves, Diego and Moussallem, Diego and Publio, Gustavo},
eprint = {1806.10478},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Soru et al. - Unknown - Neural Machine Translation for Query Construction and Composition.pdf:pdf},
mendeley-groups = {Master Thesis,Master Thesis/Background},
title = {{Neural Machine Translation for Query Construction and Composition}},
url = {https://arxiv.org/pdf/1806.10478v1.pdf http://arxiv.org/abs/1806.10478},
year = {2018}
}
@inproceedings{Trivedi2017,
abstract = {{\textcopyright} Springer International Publishing AG 2017. Being able to access knowledge bases in an intuitive way has been an active area of research over the past years. In particular, several question answering (QA) approaches which allow to query RDF datasets in natural language have been developed as they allow end users to access knowledge without needing to learn the schema of a knowledge base and learn a formal query language. To foster this research area, several training datasets have been created, e.g. in the QALD (Question Answering over Linked Data) initiative. However, existing datasets are insufficient in terms of size, variety or complexity to apply and evaluate a range of machine learning based QA approaches for learning complex SPARQL queries. With the provision of the Large-Scale Complex Question Answering Dataset (LC-QuAD), we close this gap by providing a dataset with 5000 questions and their corresponding SPARQL queries over the DBpedia dataset. In this article, we describe the dataset creation process and how we ensure a high variety of questions, which should enable to assess the robustness and accuracy of the next generation of QA systems for knowledge graphs.},
author = {Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-68204-4_22},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Trivedi et al. - Unknown - A Corpus for Complex Question Answering over Knowledge Graphs.pdf:pdf},
isbn = {9783319682037},
issn = {16113349},
mendeley-groups = {Master Thesis},
pages = {210--218},
title = {{LC-QuAD: A corpus for complex question answering over knowledge graphs}},
url = {http://sda.tech/qa-dataset/ https://figshare.com/projects/LC-QuAD/21812},
volume = {10588 LNCS},
year = {2017}
}
@inproceedings{Singh2018,
address = {New York, New York, USA},
author = {Singh, Kuldeep and Lange, Christoph and Vidal, Maria Esther and Lehmann, Jens and Auer, S{\"{o}}ren and Radhakrishna, Arun Sethupat and Both, Andreas and Shekarpour, Saeedeh and Lytra, Ioanna and Usbeck, Ricardo and Vyas, Akhilesh and Khikmatullaev, Akmal and Punjani, Dharmen},
booktitle = {Proceedings of the 2018 World Wide Web Conference on World Wide Web - WWW '18},
doi = {10.1145/3178876.3186023},
isbn = {9781450356398},
mendeley-groups = {Master Thesis},
pages = {1247--1256},
publisher = {ACM Press},
title = {{Why Reinvent the Wheel}},
url = {http://dl.acm.org/citation.cfm?doid=3178876.3186023},
year = {2018}
}
@incollection{Wang2007,
abstract = {Providing a natural language interface to ontologies will not only offer ordinary users the convenience of acquiring needed information from ontologies, but also expand the influence of ontologies and the semantic web consequently. This paper presents PANTO, a Portable nAtural laNguage inTerface to Ontologies, which accepts generic natural language queries and outputs SPARQL queries. Based on a special consideration on nominal phrases, it adopts a triple-based data model to interpret the parse trees output by an off-the-shelf parser. Complex modifications in natural language queries such as negations, superlative and comparative are investigated. The experiments have shown that PANTO provides state-of-the-art results.},
address = {Berlin, Heidelberg},
author = {Wang, Chong and Xiong, Miao and Zhou, Qi and Yu, Yong},
booktitle = {The Semantic Web: Research and Applications},
doi = {10.1007/978-3-540-72667-8_34},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2007 - PANTO A Portable Natural Language Interface to Ontologies.pdf:pdf},
isbn = {978-3-540-72666-1},
issn = {03029743},
mendeley-groups = {Master Thesis},
pages = {473--487},
publisher = {Springer Berlin Heidelberg},
title = {{PANTO: A Portable Natural Language Interface to Ontologies}},
url = {http://link.springer.com/10.1007/978-3-540-72667-8{\_}34},
year = {2007}
}
@article{Moussallem2017,
abstract = {A large number of machine translation approaches have been developed recently with the aim of migrating content easily across languages. However, the literature suggests that many obstacles must be dealt with to achieve better automatic translations. A central issue that machine translation systems must handle is ambiguity. A promising way of overcoming this problem is using semantic web technologies. This article presents the results of a systematic review of approaches that rely on semantic web technologies within machine translation approaches for translating texts. Overall, our survey suggests that while semantic web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy.},
archivePrefix = {arXiv},
arxivId = {1711.09476},
author = {Moussallem, Diego and Wauer, Matthias and Ngomo, Axel-Cyrille Ngonga},
eprint = {1711.09476},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Moussallem, Wauer, Ngonga Ngomo - Unknown - Machine Translation Using Semantic Web Technologies A Survey.pdf:pdf},
keywords = {linked data,machine translation,multilinguality,ontology,semantic web},
mendeley-groups = {Master Thesis,Master Thesis/Background},
title = {{Machine Translation Using Semantic Web Technologies: A Survey}},
url = {https://arxiv.org/pdf/1711.09476.pdf http://arxiv.org/abs/1711.09476},
year = {2017}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
doi = {abs/1609.08144},
eprint = {1609.08144v2},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {1609.08144},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Learning},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
month = {sep},
pages = {1--23},
pmid = {18319728},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@inproceedings{Soru2018a,
abstract = {In the last years, the Linked Data Cloud has achieved a size of more than 100 billion facts pertaining to a multitude of domains. However, accessing this information has been significantly challenging for lay users. Approaches to problems such as Question Answering on Linked Data and Link Discovery have notably played a role in increasing information access. These approaches are often based on handcrafted and/or statistical models derived from data observation. Recently, Deep Learning architectures based on Neural Networks called seq2seq have shown to achieve state-of-the-art results at translating sequences into sequences. In this direction, we propose Neural SPARQL Machines, end-to-end deep architectures to translate any natural language expression into sentences encoding SPARQL queries. Our preliminary results, restricted on selected DBpedia classes, show that Neural SPARQL Machines are a promising approach for Question Answering on Linked Data, as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition.},
annote = {proposed Neural SPARQL Machines, an end-to-end deep architecture to translate any natural language expression into sentences encoding SPARQL queries.
Used DBpedia in Question Answering
QA lacks good solutions for handling vocabulary mismatch and problems on graph patterns.
generator for generating training data, learner for training the model, and interpreter for interpreting model output to proper final SPARQL translation.
Used BLEU accuracy as evaluation metric
vocabulary-dependent on specific KBs
Methodology is Neural Symbolic Machines, a specific class of seq2seq model aiming at translating NL to a sequence of tokens defining a program by using a key-variable memory and reinforcement learning with weak supervision.
token-based, SPARQL operators and brackets, URIs are kept as single tokens.},
archivePrefix = {arXiv},
arxivId = {1708.07624},
author = {Soru, Tommaso and Marx, Edgard and Moussallem, Diego and Publio, Gustavo and Valdestilhas, Andr{\'{e}} and Esteves, Diego and Neto, Ciro Baron},
booktitle = {CEUR Workshop Proceedings},
eprint = {1708.07624},
issn = {16130073},
keywords = {Deep Learning,Linked Data,Machine Translation,Neural Networks,Question Answering,SPARQL},
mendeley-groups = {Master Thesis,Master Thesis/Background},
month = {aug},
title = {{SPARQL as a foreign language}},
url = {http://arxiv.org/abs/1708.07624},
volume = {2044},
year = {2018}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
annote = {- Sequence to Sequence learning
- The problem it aims to solve is DNN cannot be used to map sequence to sequence while it performs well on fixed-dimensional labeled training data
- Many problems are sequential like speech recognition, machine translation, and question answering},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - Unknown - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Advances in Neural Information Processing Systems (NIPS)},
mendeley-groups = {Master Thesis,Master Thesis/Methods},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
author = {Marcus, Mitchell P and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
doi = {10.1162/coli.2010.36.1.36100},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
mendeley-groups = {Master Project},
number = {2},
pages = {313--330},
title = {{Building a large annotated corpus of English: The Penn Treebank}},
volume = {19},
year = {1993}
}
@article{Manning2014,
abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
doi = {10.3115/v1/P14-5010},
eprint = {arXiv:1011.1669v3},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/StanfordCoreNlp2014.pdf:pdf},
isbn = {9781941643006},
issn = {1098-6596},
journal = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
mendeley-groups = {Master Project},
pages = {55--60},
pmid = {25246403},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://aclweb.org/anthology/P14-5010},
year = {2014}
}
@article{Finkel2005,
abstract = {Most current statistical natural language process- ing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sam- pling, a simple Monte Carlo method used to per- form approximate inference in factored probabilis- tic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, andCRFs, it is possible to incorpo- rate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consis- tency constraints. This technique results in an error reduction of up to 9{\%} over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/stanford - ner tagging.pdf:pdf},
issn = {02773791},
journal = {in Acl},
mendeley-groups = {Master Project},
number = {1995},
pages = {363 -- 370},
title = {{Incorporating non-local information into information extraction systems by gibbs sampling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.8904},
year = {2005}
}
@article{Koncel-kedziorski2016,
abstract = {Recent work across several AI subdisciplines has focused on automatically solving math word problems. In this paper we introduce MAWPS, an online repository of Math Word Problems, to provide a unified testbed to eval-uate different algorithms. MAWPS allows for the automatic construction of datasets with particular characteristics, providing tools for tuning the lexical and template overlap of a dataset as well as for filtering ungrammatical problems from web-sourced corpora. The on-line nature of this repository facilitates easy community contribution. At present, we have amassed 3,320 problems, including the full datasets used in several prominent works.},
author = {Koncel-kedziorski, Rik and Roy, Subhro and Amini, Aida and Kushman, Nate and Hajishirzi, Hannaneh},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/MAWPS.pdf:pdf},
isbn = {9781941643914},
journal = {Naacl-Hlt-2016},
mendeley-groups = {Master Project},
pages = {1152--1157},
title = {{MAWPS: A Math Word Problem Repository}},
year = {2016}
}
@article{Das2016,
author = {Das, Rubel and Ray, Antariksha and Mondal, Souvik and Das, Dipankar},
doi = {10.1109/ICACCI.2016.7732102},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/A Rule Based Question Generation Framework to Deal with Simple and Complex Sentences.pdf:pdf},
isbn = {9781509020287},
journal = {2016 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016},
mendeley-groups = {Master Project,Master Project/Presentation},
pages = {542--548},
title = {{A rule based question generation framework to deal with simple and complex sentences}},
year = {2016}
}
@article{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D. and Singer, Yoram},
doi = {10.3115/1073445.1073478},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/stanford - pos tagging.pdf:pdf},
journal = {Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology  - NAACL '03},
mendeley-groups = {Master Project},
pages = {173--180},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://portal.acm.org/citation.cfm?doid=1073445.1073478},
volume = {1},
year = {2003}
}
@article{Raghunathan2010,
abstract = {Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier's entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sieve-based approaches could be applied to other NLP tasks.},
author = {Raghunathan, Karthik and Lee, Heeyoung and Rangarajan, Sudarshan and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan and Manning, Christopher D.},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/stanford - coref.pdf:pdf},
isbn = {1932432868},
journal = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP '10)},
mendeley-groups = {Master Project},
number = {October},
pages = {492--501},
title = {{A Multi-Pass Sieve for Coreference Resolution}},
url = {http://dl.acm.org/citation.cfm?id=1870706},
year = {2010}
}
@incollection{Piwek2007,
address = {Berlin, Heidelberg},
author = {Piwek, Paul and Hernault, Hugo and Prendinger, Helmut and Ishizuka, Mitsuru},
booktitle = {Intelligent Virtual Agents},
doi = {10.1007/978-3-540-74997-4_16},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Piwek et al. - 2007 - T2D Generating Dialogues Between Virtual Agents Automatically from Text.pdf:pdf},
mendeley-groups = {Master Project},
pages = {161--174},
publisher = {Springer Berlin Heidelberg},
title = {{T2D: Generating Dialogues Between Virtual Agents Automatically from Text}},
url = {http://link.springer.com/10.1007/978-3-540-74997-4{\_}16},
year = {2007}
}
@article{Rus2010,
abstract = {Abstract The paper briefly describes the First Shared Task Evaluation Challenge on Question Generation that took place in Spring 2010. The campaign included two tasks : Task A- Question Generation from Paragraphs and Task B- Question Generation from Sentences ... 
},
author = {Rus, V and Wyse, B and Piwek, P and Lintean, M},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/Overview of The First Question Generation Shared Task Evaluation Challenge.pdf:pdf},
journal = {{\ldots} Language Generation {\ldots}},
keywords = {question generation,shared task evaluation campaign},
mendeley-groups = {Master Project},
title = {{The first question generation shared task evaluation challenge}},
url = {http://dl.acm.org/citation.cfm?id=1873777{\%}0Ahttp://dl.acm.org/citation.cfm?id=1873777},
year = {2010}
}
@article{Heilman2010,
abstract = {We address the problem of automatically generating concise factual questions from linguistically complex sentences in reading ma- terials. We discuss semantic and pragmatic issues that appear in com- plex sentences, and then we present an algorithm for extracting simpli- ed sentences from appositives, subordinate clauses, and other construc- tions. We conjecture that our method is useful as a preliminary step in a larger question generation process. Experimental results indicate that our method is more suitable for factual question generation applications than an alternative text compression algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.06737},
author = {Heilman, M and Smith, N.},
eprint = {1606.06737},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/Extracting Simplified Statements for Factual Question Generation.pdf:pdf},
issn = {10495258},
journal = {{\ldots} the 3rd Workshop on Question Generation},
mendeley-groups = {Master Project,Master Project/Presentation},
pages = {11--20},
title = {{Extracting simplified statements for factual question generation}},
url = {http://141.225.40.110/papers/QG2010/PDFs/Michael Heilman - Extracting Simplified Statements.pdf},
year = {2010}
}
@article{Piwek2010,
author = {Piwek, Paul and Boyer, Kirsty Elizabeth},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/The Third Workshop on Question Generation.pdf:pdf},
journal = {At The Tenth International Conference on Intelligent Tutoring Systems (ITS2010)},
mendeley-groups = {Master Project},
pages = {91},
title = {{Proceedings of QG2010 : The Third Workshop on Question Generation}},
year = {2010}
}
@article{Ali2010,
abstract = {Question Generation (QG) and Question Answering (QA) are key challenges facing systems that interact with natural languages. The potential benefits of using automated systems to generate questions helps reduce the dependency on humans to generate questions and other needs associated with systems interacting with natural languages. In this paper we consider a system that automates generation of questions from a sentence, given a sentence, the system, will generate all possible questions which this sentence contain these questions answers. Since the given sentence may be a complex sentence, the system will generate elementary sentences, from the input complex sentences, using a syntactic parser. A part of speech tagger and a named entity recogniser are used to encode needed information. Based on the subject, verb, object and preposition the sentence will be classified, in order determine the type of questions that can possibly be generated from this sentence. We use development data provided by the Question Generation Shared Task Evaluation Challenge 2010. Keywords:},
author = {Ali, H and Chali, Y and Hasan, S.},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/已用/Automation of Question Generation From Sentences.pdf:pdf},
journal = {Proceedings of QG2010: The Third Workshop on Question Generation},
keywords = {Elementary Sentence,Named Entity Tagging,POS Tagging,Question Generation,Recall.,Syntactic Parsing},
mendeley-groups = {Master Project},
pages = {58--67},
title = {{Automation of Question Generation from Sentences}},
year = {2010}
}
@article{Heilman2011,
abstract = {Texts with potential educational value are becoming available through the Internet (e.g.,Wikipedia, news services). However, using these new texts in classrooms introduces many challenges, one of which is that they usually lack practice exercises and assessments. Here, we address part of this chal- lenge by automating the creation of a specific type of assessment item. Specifically, we focus on automatically generating factual WH questions. Our goal is to create an automated system that can take as input a text and produce as output questions for assessing a reader's knowledge of the information in the text. The questions could then be presented to a teacher, who could select and revise the ones that he or she judges to be useful. After introducing the problem, we describe some of the computational and linguistic challenges presented by factual question generation. We then present an implemented system that leverages ex- isting natural language processing techniques to address some of these challenges. The system uses a combination of manually encoded transformation rules and a statistical question ranker trained on a tailored dataset of labeled system output. We present experiments that evaluate individual components of the system as well as the system as a whole. We found, among other things, that the question ranker roughly doubled the acceptability rate of top-ranked questions. In a user study, we tested whether K-12 teachers could efficiently create factual questions by se- lecting and revising suggestions from the system. Offering automatic suggestions reduced the time and effort spent by participants, though it also affected the types of questions that were created. This research supports the idea that natural language processing can help teachers efficiently cre- ate instructional content. It provides solutions to some of the major challenges in question generation and an analysis and better understanding of those that remain.},
author = {Heilman, Michael},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/heilman-question-generation-dissertation.pdf:pdf},
isbn = {978-1-267-58224-9},
journal = {Dissertation},
mendeley-groups = {Master Project},
number = {June},
pages = {203},
pmid = {1056666460},
title = {{Automatic Factual Question Generation from Text}},
url = {www.lti.cs.cmu.edu},
year = {2011}
}
@article{Le2014,
author = {Le, Nguyen-Thinh and Kojiri, Tomoko and Pinkwart, Niels},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/Automatic Question Generation for Educational Applications – The State of Art.pdf:pdf},
journal = {Advanced Computational Methods for Knowledge Engineering},
keywords = {QG},
mendeley-groups = {Master Project},
pages = {325--338},
title = {{Automatic Question Generation for Educational Applications--The State of Art}},
year = {2014}
}
@article{Heilman2010,
abstract = {We address the challenge of automatically generating questions from reading materials for educational practice and assessment. Our approach is to overgenerate questions, then rank them. We use manually written rules to perform a sequence of general purpose syntactic transformations (e.g., subject-auxiliary inversion) to turn declarative sentences into questions. These questions are then ranked by a logistic regression model trained on a small, tailored dataset consisting of labeled output from our system. Experimental results show that ranking nearly doubles the percentage of questions rated as acceptable by annotators, from 27{\%} of all questions to 52{\%} of the top ranked 20{\%} of questions. {\textcopyright} 2010 Association for Computational Linguistics.},
author = {Heilman, Michael and Smith, Noah A.},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/Good Question! Statistical Ranking for Question Generation.pdf:pdf},
isbn = {1932432655},
journal = {NAACL HLT 2010 - Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference},
mendeley-groups = {Master Project},
number = {June},
pages = {609--617},
title = {{Good question! Statistical ranking for question generation}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-80054692957{\&}partnerID=tZOtx3y1},
year = {2010}
}
@article{Indurthi2017,
abstract = {In recent years, knowledge graphs such as Freebase that capture facts about enti-ties and relationships between them have been used actively for answering factoid questions. In this paper, we explore the problem of automatically generating ques-tion answer pairs from a given knowl-edge graph. The generated question an-swer (QA) pairs can be used in several downstream applications. For example, they could be used for training better QA systems. To generate such QA pairs, we first extract a set of keywords from enti-ties and relationships expressed in a triple stored in the knowledge graph. From each such set, we use a subset of keywords to generate a natural language question that has a unique answer. We treat this sub-set of keywords as a sequence and pro-pose a sequence to sequence model using RNN to generate a natural language ques-tion from it. Our RNN based model gen-erates QA pairs with an accuracy of 33.61 percent and performs 110.47 percent (rel-ative) better than a state-of-the-art tem-plate based method for generating natu-ral language question from keywords. We also do an extrinsic evaluation by using the generated QA pairs to train a QA sys-tem and observe that the F1-score of the QA system improves by 5.5 percent (rela-tive) when using automatically generated QA pairs in addition to manually gener-ated QA pairs available for training.},
author = {Indurthi, Sathish and Raghu, Dinesh and Khapra, Mitesh M and Joshi, Sachindra and Electronics, Samsung},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/E17-1036.pdf:pdf},
isbn = {9781510838604},
journal = {Eacl},
mendeley-groups = {Master Project},
pages = {376--385},
title = {{Generating Natural Language Question-Answer Pairs from a Knowledge Graph Using a RNN Based Question Generation Model}},
url = {https://aclanthology.info/pdf/E/E17/E17-1036.pdf{\%}0Ahttps://www.aclweb.org/anthology/E/E17/E17-1036.pdf},
volume = {1},
year = {2017}
}
@inproceedings{10.1007/978-3-319-39583-8_3,
abstract = {Questioning has been shown to improve learning outcomes, and automatic question generation can greatly facilitate the inclusion of questions in learning technologies such as intelligent tutoring systems. The majority of prior QG systems use parsing software and transformation algorithms to create questions. In contrast, the approach described here infuses natural language understanding (NLU) into the natural language generation (NLG) process by first analyzing the central semantic content of each independent clause in each sentence. Then question templates are matched to what the sentence is communicating in order to generate higher quality questions. This approach generated a higher percentage of acceptable questions than prior state-of-the-art systems.},
address = {Cham},
author = {Mazidi, Karen and Tarau, Paul},
booktitle = {Intelligent Tutoring Systems},
editor = {Micarelli, Alessandro and Stamper, John and Panourgia, Kitty},
isbn = {978-3-319-39583-8},
mendeley-groups = {Master Project},
pages = {23--33},
publisher = {Springer International Publishing},
title = {{Automatic Question Generation: From NLU to NLG}},
year = {2016}
}
@misc{Bobrow1964,
abstract = {This paper describes a computer program which accepts and «understands» a comfortable, but restricted set of one natural language, English. Certain difficulties are inherent in this problem of making a machine «understand» English. Within the limited framework of the subject matter understood by the program, many of these problems are solved or circumvented. I shall describe these problems and my solutions, and point out those solutions which I feel have general applicability. I will also indicate which must be replaced by more general methods to be really useful, and give my ideas about what general solutions to these particular problems might entail.},
author = {Bobrow, Daniel G.},
booktitle = {Artificial Intelligence Project Memo 66},
doi = {1721.1/5922},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/Natural Language Input For a Computer Problem Solving System.pdf:pdf},
keywords = {first theses},
mendeley-groups = {Master Project},
title = {{Natural Language Input for a Computer Problem Solving System}},
url = {http://dspace.mit.edu/bitstream/handle/1721.1/5922/AIM-066.pdf?sequence=2},
year = {1964}
}
@article{Chen2016,
abstract = {The current state-of-the-art method for generating educational content, such as math word problems and hints, is manual authoring by domain experts. Unfortunately, this is costly, time consuming, and produces content that lacks diversity. Attempts to automatically address the time and diversity issues through natural language generation still do not produce content that is sufficiently creative and varied. Crowdsourcing is a viable alternative - there has been a great deal of research on leveraging human creativity to solve complex problems, such as user interface design. However, these systems typically decompose complex tasks into subtasks. Writing a single word problem or hint is a small enough problem that it is unclear how to further break it down, but also far more complex than typical microtasks like image labeling. Therefore, it is not obvious how to apply these worker improvement methods or which ones are most effective (if at all). We build upon successful task design factors in prior work and run a series of iterative studies, incrementally adding different worker-support elements. Our results show that successive task designs improved accuracy and creativity.},
author = {Chen, Yvonne and Mandel, Travis and Liu, Yun-En and Popovi{\'{c}}, Zoran},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2016 - Crowdsourcing Accurate and Creative Word Problems and Hints.pdf:pdf},
journal = {Fourth AAAI Conference on Human Computation and Crowdsourcing},
mendeley-groups = {Master Project},
pages = {12--21},
title = {{Crowdsourcing Accurate and Creative Word Problems and Hints}},
url = {https://aaai.org/ocs/index.php/HCOMP/HCOMP16/paper/viewFile/14053/13628{\%}0Ahttps://aaai.org/ocs/index.php/HCOMP/HCOMP16/paper/view/14053},
year = {2016}
}
@article{Williams2011,
abstract = {This paper describes a prototype system that generates mathematical word problems from ontologies in unre-stricted domains. It builds on an existing ontology ver-baliser that renders logical statements written in Web Ontology Language (OWL) as English sentences. This kind of question is more complex than those normally attempted by question generation systems, since math-ematical word problems consist of a number of sen-tences that communicate a short narrative (in addition to providing the relevant numerical information required to solve the underlying mathematical problem). Thus, they embody many research issues that do not crop up with single-sentence questions. As well as describing the prototype system, I discuss five ways in which the difficulty of the generated questions may be controlled automatically during generation.},
author = {Williams, Sandra},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Williams - 2011 - Generating Mathematical Word Problems.pdf:pdf},
isbn = {9781577355489},
journal = {2011 AAAI Fall Symposium},
keywords = {Technical Report FS-11-04},
mendeley-groups = {Master Project},
pages = {61--64},
title = {{Generating Mathematical Word Problems}},
year = {2011}
}
@article{Polozov2015,
abstract = {Word problems are an established technique for teaching mathematical modeling skills in K-12 ed- ucation. However, many students find word prob- lems unconnected to their lives, artificial, and un- interesting. Most students find them much more difficult than the corresponding symbolic represen- tations. To account for this phenomenon, an ideal pedagogy might involve an individually crafted progression of unique word problems that form a personalized plot. We propose a novel technique for automatic gen- eration of personalized word problems. In our system, word problems are generated from gen- eral specifications using answer-set programming (ASP). The specifications include tutor require- ments (properties of a mathematical model), and student requirements (personalization, characters, setting). Our system takes a logical encoding of the specification, synthesizes a word problem narrative and its mathematical model as a labeled logical plot graph, and realizes the problem in natural language. Human judges found our problems as solvable as the textbook problems, with a slightly more artificial language.},
author = {Polozov, Oleksandr and O'Rourke, Eleanor and Smith, Adam M. and Zettlemoyer, Luke and Gulwani, Sumit and Popovi{\'{c}}, Zoran},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Polozov et al. - 2015 - Personalized mathematical word problem generation.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
mendeley-groups = {Master Project},
pages = {381--388},
title = {{Personalized mathematical word problem generation}},
volume = {2015-Janua},
year = {2015}
}
@article{Mukherjee2008,
abstract = {This article addresses the problem of understanding mathematics described in natural language. Research in this area dates back to early 1960s. Several systems have so far been proposed to involve machines to solve mathematical problems of various domains like algebra, geometry, physics, mechanics, etc. This correspondence provides a state of the art technical review of these systems and approaches proposed by different research groups. A unified architecture that has been used in most of these approaches is identified and differences among the systems are highlighted. Significant achievements of each method are pointed out. Major strengths and weaknesses of the approaches are also discussed. Finally, present efforts and future trends in this research area are presented.},
author = {Mukherjee, Anirban and Garain, Utpal},
doi = {10.1007/s10462-009-9110-0},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/A review of methods for automatic understanding of natural language mathematical problems.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Artificial intelligence,Automated reasoning,Knowledge engineering,Mathematical problems,Natural language processing},
mendeley-groups = {Master Project},
number = {2},
pages = {93--122},
title = {{A review of methods for automatic understanding of natural language mathematical problems}},
volume = {29},
year = {2008}
}
@article{Koncel-Kedziorski2016,
abstract = {Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called {\{}$\backslash$it rewriting{\}} that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a two-stage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes.},
archivePrefix = {arXiv},
arxivId = {1610.06210},
author = {Koncel-Kedziorski, Rik and Konstas, Ioannis and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
eprint = {1610.06210},
file = {:Users/yin/Google Drive/TUD Master/Project/Reference/A Theme-Rewriting Approach for Generating Algebra Word Problems.pdf:pdf},
mendeley-groups = {Master Project},
title = {{A Theme-Rewriting Approach for Generating Algebra Word Problems}},
url = {http://arxiv.org/abs/1610.06210},
year = {2016}
}
