@article{Mikolov2017,
abstract = {Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.},
archivePrefix = {arXiv},
arxivId = {1712.09405},
author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
eprint = {1712.09405},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2017 - Advances in Pre-Training Distributed Word Representations.pdf:pdf},
keywords = {fastText pre-trained vectors},
mendeley-groups = {Master Thesis},
mendeley-tags = {fastText pre-trained vectors},
month = {dec},
title = {{Advances in Pre-Training Distributed Word Representations}},
url = {http://arxiv.org/abs/1712.09405},
year = {2017}
}
@article{Luong2015,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
doi = {10.18653/v1/D15-1166},
eprint = {1508.04025},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Luong, Pham, Manning - 2015 - Effective Approaches to Attention-based Neural Machine Translation.pdf:pdf},
isbn = {9781941643327},
issn = {10495258},
journal = {EMNLP},
keywords = {()},
mendeley-groups = {Master Thesis},
number = {September},
pages = {11},
pmid = {14527267},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {http://nlp.stanford.edu/projects/nmt. http://arxiv.org/abs/1508.04025},
year = {2015}
}
@inproceedings{Sorokin2017,
abstract = {In this paper we present a factoid question answering system for participation in Task 4 of the QALD-7 shared task. Our system is an end-to-end neural architecture for learning a semantic representation of the input question. It iteratively generates representations and uses a convolutional neural network (CNN) model to score them at each step. We take the semantic representation with the highest final score and execute it against Wikidata to retrieve the answers. We show on the Task 4 data set that our system is able to successfully generalize to new data.},
author = {Sorokin, Daniil and Gurevych, Iryna},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-319-69146-6_7},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Sorokin, Gurevych - 2017 - End-to-end representation learning for question answering with weak supervision.pdf:pdf},
isbn = {9783319691459},
issn = {18650929},
keywords = {Convolutional neural networks,Question-answering,Representation learning,Semantic parsing,Semantic web,Weak supervision},
mendeley-groups = {Master Thesis},
pages = {70--83},
publisher = {Springer, Cham},
title = {{End-to-end representation learning for question answering with weak supervision}},
url = {http://link.springer.com/10.1007/978-3-319-69146-6{\_}7},
volume = {769},
year = {2017}
}
@misc{Lake2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
booktitle = {Behavioral and Brain Sciences},
doi = {10.1017/S0140525X16001837},
eprint = {1604.00289},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf:pdf},
isbn = {9781577357384},
issn = {14691825},
mendeley-groups = {Master Thesis},
month = {jul},
pages = {1--101},
pmid = {1000303116},
title = {{Building Machines That Learn and Think Like People}},
url = {http://arxiv.org/abs/1607.04606},
year = {2016}
}
@misc{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
booktitle = {Nature},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {14764687},
keywords = {Computer science,Mathematics and computing},
mendeley-groups = {Master Thesis},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@inproceedings{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NIPS)},
doi = {v5},
eprint = {1503.08895},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Sukhbaatar et al. - 2015 - End-To-End Memory Networks.pdf:pdf},
isbn = {1551-6709},
issn = {10495258},
mendeley-groups = {Master Thesis},
month = {mar},
pmid = {9377276},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {1410.5401},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
isbn = {0028-0836},
issn = {2041-1723},
mendeley-groups = {Master Thesis},
month = {oct},
pmid = {18958277},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
mendeley-groups = {Master Thesis},
month = {sep},
pmid = {14527267},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2014}
}
@article{Gehring2017,
abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
archivePrefix = {arXiv},
arxivId = {1705.03122},
author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
doi = {10.18653/v1/P16-1220},
eprint = {1705.03122},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Gehring et al. - Unknown - Convolutional Sequence to Sequence Learning.pdf:pdf},
isbn = {9781510827585},
issn = {1938-7228},
mendeley-groups = {Master Thesis},
title = {{Convolutional Sequence to Sequence Learning}},
url = {https://arxiv.org/pdf/1705.03122.pdf http://arxiv.org/abs/1705.03122},
year = {2017}
}
@article{Soru2018,
abstract = {Research on question answering with knowledge base has recently seen an increasing use of deep architectures. In this extended abstract, we study the application of the neural machine translation paradigm for question parsing. We employ a sequence-to-sequence model to learn graph patterns in the SPARQL graph query language and their compositions. Instead of inducing the programs through question-answer pairs, we expect a semi-supervised approach, where alignments between questions and queries are built through templates. We argue that the coverage of language utterances can be expanded using late notable works in natural language generation.},
archivePrefix = {arXiv},
arxivId = {1806.10478},
author = {Soru, Tommaso and Marx, Edgard and Valdestilhas, Andr{\'{e}} and Esteves, Diego and Moussallem, Diego and Publio, Gustavo},
eprint = {1806.10478},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Soru et al. - Unknown - Neural Machine Translation for Query Construction and Composition.pdf:pdf},
mendeley-groups = {Master Thesis},
title = {{Neural Machine Translation for Query Construction and Composition}},
url = {https://arxiv.org/pdf/1806.10478v1.pdf http://arxiv.org/abs/1806.10478},
year = {2018}
}
@inproceedings{Trivedi2017,
abstract = {{\textcopyright} Springer International Publishing AG 2017. Being able to access knowledge bases in an intuitive way has been an active area of research over the past years. In particular, several question answering (QA) approaches which allow to query RDF datasets in natural language have been developed as they allow end users to access knowledge without needing to learn the schema of a knowledge base and learn a formal query language. To foster this research area, several training datasets have been created, e.g. in the QALD (Question Answering over Linked Data) initiative. However, existing datasets are insufficient in terms of size, variety or complexity to apply and evaluate a range of machine learning based QA approaches for learning complex SPARQL queries. With the provision of the Large-Scale Complex Question Answering Dataset (LC-QuAD), we close this gap by providing a dataset with 5000 questions and their corresponding SPARQL queries over the DBpedia dataset. In this article, we describe the dataset creation process and how we ensure a high variety of questions, which should enable to assess the robustness and accuracy of the next generation of QA systems for knowledge graphs.},
author = {Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-68204-4_22},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Trivedi et al. - Unknown - A Corpus for Complex Question Answering over Knowledge Graphs.pdf:pdf},
isbn = {9783319682037},
issn = {16113349},
mendeley-groups = {Master Thesis},
pages = {210--218},
title = {{LC-QuAD: A corpus for complex question answering over knowledge graphs}},
url = {http://sda.tech/qa-dataset/ https://figshare.com/projects/LC-QuAD/21812},
volume = {10588 LNCS},
year = {2017}
}
@inproceedings{Singh2018,
address = {New York, New York, USA},
author = {Singh, Kuldeep and Lange, Christoph and Vidal, Maria Esther and Lehmann, Jens and Auer, S{\"{o}}ren and Radhakrishna, Arun Sethupat and Both, Andreas and Shekarpour, Saeedeh and Lytra, Ioanna and Usbeck, Ricardo and Vyas, Akhilesh and Khikmatullaev, Akmal and Punjani, Dharmen},
booktitle = {Proceedings of the 2018 World Wide Web Conference on World Wide Web - WWW '18},
doi = {10.1145/3178876.3186023},
isbn = {9781450356398},
mendeley-groups = {Master Thesis},
pages = {1247--1256},
publisher = {ACM Press},
title = {{Why Reinvent the Wheel}},
url = {http://dl.acm.org/citation.cfm?doid=3178876.3186023},
year = {2018}
}
@incollection{Wang2007,
abstract = {Providing a natural language interface to ontologies will not only offer ordinary users the convenience of acquiring needed information from ontologies, but also expand the influence of ontologies and the semantic web consequently. This paper presents PANTO, a Portable nAtural laNguage inTerface to Ontologies, which accepts generic natural language queries and outputs SPARQL queries. Based on a special consideration on nominal phrases, it adopts a triple-based data model to interpret the parse trees output by an off-the-shelf parser. Complex modifications in natural language queries such as negations, superlative and comparative are investigated. The experiments have shown that PANTO provides state-of-the-art results.},
address = {Berlin, Heidelberg},
author = {Wang, Chong and Xiong, Miao and Zhou, Qi and Yu, Yong},
booktitle = {The Semantic Web: Research and Applications},
doi = {10.1007/978-3-540-72667-8_34},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2007 - PANTO A Portable Natural Language Interface to Ontologies.pdf:pdf},
isbn = {978-3-540-72666-1},
issn = {03029743},
mendeley-groups = {Master Thesis},
pages = {473--487},
publisher = {Springer Berlin Heidelberg},
title = {{PANTO: A Portable Natural Language Interface to Ontologies}},
url = {http://link.springer.com/10.1007/978-3-540-72667-8{\_}34},
year = {2007}
}
@article{Moussallem2017,
abstract = {A large number of machine translation approaches have been developed recently with the aim of migrating content easily across languages. However, the literature suggests that many obstacles must be dealt with to achieve better automatic translations. A central issue that machine translation systems must handle is ambiguity. A promising way of overcoming this problem is using semantic web technologies. This article presents the results of a systematic review of approaches that rely on semantic web technologies within machine translation approaches for translating texts. Overall, our survey suggests that while semantic web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy.},
archivePrefix = {arXiv},
arxivId = {1711.09476},
author = {Moussallem, Diego and Wauer, Matthias and Ngomo, Axel-Cyrille Ngonga},
eprint = {1711.09476},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Moussallem, Wauer, Ngonga Ngomo - Unknown - Machine Translation Using Semantic Web Technologies A Survey.pdf:pdf},
keywords = {linked data,machine translation,multilinguality,ontology,semantic web},
mendeley-groups = {Master Thesis},
title = {{Machine Translation Using Semantic Web Technologies: A Survey}},
url = {https://arxiv.org/pdf/1711.09476.pdf http://arxiv.org/abs/1711.09476},
year = {2017}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
doi = {abs/1609.08144},
eprint = {1609.08144v2},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
isbn = {1471-0048 (Electronic)$\backslash$r1471-003X (Linking)},
issn = {1609.08144},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Learning},
mendeley-groups = {Master Thesis},
month = {sep},
pages = {1--23},
pmid = {18319728},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@inproceedings{Soru2018a,
abstract = {In the last years, the Linked Data Cloud has achieved a size of more than 100 billion facts pertaining to a multitude of domains. However, accessing this information has been significantly challenging for lay users. Approaches to problems such as Question Answering on Linked Data and Link Discovery have notably played a role in increasing information access. These approaches are often based on handcrafted and/or statistical models derived from data observation. Recently, Deep Learning architectures based on Neural Networks called seq2seq have shown to achieve state-of-the-art results at translating sequences into sequences. In this direction, we propose Neural SPARQL Machines, end-to-end deep architectures to translate any natural language expression into sentences encoding SPARQL queries. Our preliminary results, restricted on selected DBpedia classes, show that Neural SPARQL Machines are a promising approach for Question Answering on Linked Data, as they can deal with known problems such as vocabulary mismatch and perform graph pattern composition.},
archivePrefix = {arXiv},
arxivId = {1708.07624},
author = {Soru, Tommaso and Marx, Edgard and Moussallem, Diego and Publio, Gustavo and Valdestilhas, Andr{\'{e}} and Esteves, Diego and Neto, Ciro Baron},
booktitle = {CEUR Workshop Proceedings},
eprint = {1708.07624},
issn = {16130073},
keywords = {Deep Learning,Linked Data,Machine Translation,Neural Networks,Question Answering,SPARQL},
mendeley-groups = {Master Thesis},
month = {aug},
title = {{SPARQL as a foreign language}},
url = {http://arxiv.org/abs/1708.07624},
volume = {2044},
year = {2018}
}
@article{Luz2018,
abstract = {Semantic parsing is the process of mapping a natural language sentence into a formal representation of its meaning. In this work we use the neural network approach to transform natural language sentence into a query to an ontology database in the SPARQL language. This method does not rely on handcraft-rules, high-quality lexicons, manually-built templates or other handmade complex structures. Our approach is based on vector space model and neural networks. The proposed model is based in two learning steps. The first step generates a vector representation for the sentence in natural language and SPARQL query. The second step uses this vector representation as input to a neural network (LSTM with attention mechanism) to generate a model able to encode natural language and decode SPARQL.},
archivePrefix = {arXiv},
arxivId = {1803.04329},
author = {Luz, Fabiano Ferreira and Finger, Marcelo},
eprint = {1803.04329},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Ferreira Luz, Finger - Unknown - Semantic Parsing Natural Language into SPARQL Improving Target Language Representation with Neural Atte.pdf:pdf},
mendeley-groups = {Master Thesis},
title = {{Semantic Parsing Natural Language into SPARQL: Improving Target Language Representation with Neural Attention}},
url = {https://arxiv.org/pdf/1803.04329.pdf http://arxiv.org/abs/1803.04329},
year = {2018}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:Users/yin/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - Unknown - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Advances in Neural Information Processing Systems (NIPS)},
mendeley-groups = {Master Thesis},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
